# ç¬¬ 2 ç« ï¼šè§£å¯† AI åŠ é€Ÿå™¨â€”â€”ä»è½¯ä»¶æ ˆåˆ°ç¡¬ä»¶æ¶æ„

<div align='center'>

[![AMD](https://img.shields.io/badge/AMD-ROCm7.2-ED1C24)](https://rocm.docs.amd.com/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.x-EE4C2C)](https://pytorch.org/)
[![GPU](https://img.shields.io/badge/GPU-Radeon_8060S-orange)]()
[![Arch](https://img.shields.io/badge/Arch-gfx1151-blue)]()

</div>

> **ğŸ–¥ï¸ å®éªŒç¯å¢ƒ**
> - **è®¾å¤‡**: AMD AI+ MAX395
> - **GPU**: Radeon 8060S
> - **æ¶æ„**: gfx1151 (RDNA 3)
> - **ROCm ç‰ˆæœ¬**: 7.x
> - **ç³»ç»Ÿ**: Ubuntu 24.04 / 22.04

---

## ğŸ¯ æœ¬ç« å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬ç« ï¼Œä½ å°†ç†è§£ä¸‰ä»¶æ ¸å¿ƒäº‹æƒ…ï¼š

1. âœ… **è½¯ä»¶è°ƒç”¨é“¾è·¯**ï¼šPyTorch ä»£ç å¦‚ä½•ç»è¿‡ HIP â†’ HSA â†’ Driver â†’ GPU æ‰§è¡Œ
2. âœ… **æ€ç»´èŒƒå¼è½¬å˜**ï¼šä» CPU çš„"ä½å»¶è¿Ÿ"åˆ° GPU çš„"é«˜åå"ï¼ŒSIMT æ¨¡å‹çš„å·¥ä½œåŸç†
3. âœ… **ç¡¬ä»¶æ¶æ„åŸç†**ï¼šAMD GPU çš„ CUã€LDSã€HBMï¼Œä»¥åŠå†…å­˜å¸¦å®½çš„é‡è¦æ€§

***OKï¼Œè®©æˆ‘ä»¬å¼€å§‹æ·±å…¥æ¢ç´¢ AI åŠ é€Ÿå™¨çš„å¥¥ç§˜å§ï¼***

---

## 2.1 ä» Python åˆ° GPUï¼šä¸€æ¬¡ä»£ç æ‰§è¡Œçš„å®Œæ•´æ—…ç¨‹

å½“ä½ å†™ä¸‹ `x + y` è¿™æ ·çš„ PyTorch ä»£ç æ—¶ï¼Œä½ çŸ¥é“è¿™è¡Œä»£ç ç»å†äº†å¤šå°‘å±‚"ç¿»è¯‘"æ‰æœ€ç»ˆåœ¨ GPU ä¸Šæ‰§è¡Œå—ï¼Ÿè¿™ä¸€èŠ‚ï¼Œæˆ‘ä»¬ç”¨ Linux çš„å·¥å…·æ¥è¿½è¸ªæ•´ä¸ªè°ƒç”¨é“¾è·¯ã€‚

### 2.1.1 é»‘ç›’è§£å¯†ï¼šç”¨ `ldd` è¿½è¸ª PyTorch çš„ä¾èµ–é“¾è·¯

> ğŸ’¡ **å°è´´å£«**ï¼šPyTorch åªæ˜¯ä¸€ä¸ªé«˜å±‚å°è£…ï¼ŒçœŸæ­£åœ¨ GPU ä¸Šå¹²æ´»çš„æ˜¯åº•å±‚çš„ ROCm è½¯ä»¶æ ˆã€‚è®©æˆ‘ä»¬ç”¨ `ldd` å·¥å…·ï¼ˆæŸ¥çœ‹åŠ¨æ€åº“ä¾èµ–ï¼‰æ¥æ­å¼€è¿™ä¸ªé»‘ç›’ã€‚

#### ğŸ” è¿½è¸ªå‘½ä»¤

```bash
# æ‰¾åˆ° torch åº“çš„è·¯å¾„
TORCH_LIB=$(python -c "import torch; print(torch.__file__)" | sed 's/__init__.py/lib/')

# æŸ¥çœ‹æ ¸å¿ƒä¾èµ– (è¿‡æ»¤å‡º amd/rocm ç›¸å…³çš„)
ldd $TORCH_LIB/libtorch_python.so | grep -E "amd|hip|hsa"
```

#### ğŸ“Š è¾“å‡ºç¤ºä¾‹

![alt text](images/2.ldd_image.png)

è¿™äº›åº“æ„æˆäº† **ROCm è½¯ä»¶æ ˆçš„æ ¸å¿ƒ**ï¼Œæˆ‘ä»¬é€ä¸ªæ‹†è§£ï¼š

#### ğŸ”§ å››å¤§æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | åº“å | èŒè´£ | å¯¹åº”å…³ç³» |
|:---|:---|:---|:---|
| **ç¿»è¯‘å®˜** | `libamdhip64.so` | å°† CUDA é£æ ¼çš„ API è°ƒç”¨è½¬æ¢ä¸º AMD çš„æŒ‡ä»¤ | NVIDIA çš„ `libcudart` |
| **å·¥å¤´** | `libhsa-runtime64.so` | çœŸæ­£è°ƒåº¦ GPUã€ç®¡ç†å†…å­˜ã€è®©æ˜¾å¡å¼€å§‹å¹²æ´» | HSA å¼‚æ„è®¡ç®—åŸºç¡€æ¶æ„ |
| **æŠ€èƒ½åŒ…** | `hipblas`/`hipfft` ç­‰ | é«˜æ€§èƒ½æ•°å­¦åº“ï¼ˆçŸ©é˜µä¹˜æ³•ã€FFT ç­‰ï¼‰ | NVIDIA çš„ cuBLAS/cuFFT |
| **ç¼–è¯‘å™¨å‰ç«¯** | `libamd_comgr.so` | åŠ¨æ€ç¼–è¯‘ HIP ä»£ç ä¸ºäºŒè¿›åˆ¶å¯¹è±¡ | NVIDIA çš„ NVRTC |

#### ğŸ“š æ•°å­¦åº“è¯¦è§£

| åº“å | ä½œç”¨ | åº”ç”¨åœºæ™¯ |
|:---|:---|:---|
| `hipblas` | çŸ©é˜µè¿ç®—ï¼ˆBLASï¼‰ | çº¿æ€§å±‚ã€çŸ©é˜µä¹˜æ³• |
| `hipfft` | å¿«é€Ÿå‚…é‡Œå¶å˜æ¢ | ä¿¡å·å¤„ç†ã€æŸäº›æ³¨æ„åŠ›æœºåˆ¶ |
| `hiprand` | éšæœºæ•°ç”Ÿæˆ | Dropoutã€å™ªå£°æ³¨å…¥ |
| `hipsparse` | ç¨€ç–çŸ©é˜µè¿ç®— | ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ |

<div style="background: #e3f2fd; border: 1px solid #2196f3; border-radius: 8px; padding: 16px; margin: 16px 0;">
  <div style="display: flex; align-items: start;">
    <span style="font-size: 20px; margin-right: 10px;">ğŸ’¡</span>
    <div>
      <strong style="color: #1565c0;">ä¸ºä»€ä¹ˆéœ€è¦è¿™äº›æ•°å­¦åº“ï¼Ÿ</strong><br>
      <span style="color: #1565c0; line-height: 1.6;">
        è¿™äº›åº“æ˜¯ AMD å·¥ç¨‹å¸ˆç”¨æ±‡ç¼–è¯­è¨€æ‰‹å†™ä¼˜åŒ–çš„ï¼Œæ€§èƒ½æ¯”ä½ è‡ªå·±å†™çš„ HIP ä»£ç å¿« 10-100 å€ã€‚å½“ä½ è·‘ Qwen æ¨¡å‹æ—¶ï¼Œå¤§é‡çš„çŸ©é˜µè¿ç®—å°±æ˜¯ç”± <code>hipblas</code> å®Œæˆçš„ã€‚
      </span>
    </div>
  </div>
</div>

---

### 2.1.2 å…¨æ™¯å›¾è§£ï¼šå®Œæ•´è°ƒç”¨é“¾è·¯

ç°åœ¨æˆ‘ä»¬æŠŠæ•´ä¸ªè°ƒç”¨é“¾è·¯ç”¨ Mermaid æµç¨‹å›¾ç”»å‡ºæ¥ï¼š

```mermaid
graph TB
    subgraph "åº”ç”¨å±‚"
        A1[ä½ çš„ Python ä»£ç <br/>x = torch.randn 1000,1000<br/>z = x @ y]
    end

    subgraph "PyTorch æ¡†æ¶å±‚"
        B1[PyTorch Python å±‚<br/>è§£æå¯¹è±¡ã€æŸ¥æ‰¾å®ç°]
        B2[PyTorch C++ å±‚ ATen<br/>è°ƒç”¨ HIP BLASã€å‡†å¤‡æ•°æ®]
    end

    subgraph "ROCm è¿è¡Œæ—¶å±‚"
        C1[HIP Runtime<br/>libamdhip64.so<br/>è½¬æ¢ APIã€ç®¡ç†å†…å­˜]
        C2[HSA Runtime<br/>libhsa-runtime64.so<br/>åˆ›å»ºé˜Ÿåˆ—ã€åˆ†é…æ˜¾å­˜ã€å¯åŠ¨è®¡ç®—]
    end

    subgraph "é©±åŠ¨ä¸ç¡¬ä»¶å±‚"
        D1[AMDGPU Driver<br/>æäº¤å‘½ä»¤åˆ°ç¡¬ä»¶é˜Ÿåˆ—]
        D2[GPU Hardware<br/>Compute Units æ‰§è¡Œ<br/>HBM æ˜¾å­˜æä¾›æ•°æ®]
    end

    A1 --> B1
    B1 --> B2
    B2 --> C1
    C1 --> C2
    C2 --> D1
    D1 --> D2

    style A1 fill:#e3f2fd
    style B1 fill:#fff3e0
    style B2 fill:#fff3e0
    style C1 fill:#f3e5f5
    style C2 fill:#f3e5f5
    style D1 fill:#e8f5e9
    style D2 fill:#c8e6c9
```

#### ğŸ”„ å…³é”®æ•°æ®æµ

| é˜¶æ®µ | ä½ç½® | ä»»åŠ¡ |
|:---|:---|:---|
| **1. CPU ç«¯** | ç³»ç»Ÿå†…å­˜ | å‡†å¤‡æ•°æ®ã€è°ƒç”¨ API |
| **2. PCIe æ€»çº¿** | æ€»çº¿ä¼ è¾“ | æ•°æ®ä»ç³»ç»Ÿå†…å­˜æ¬è¿åˆ°æ˜¾å­˜ |
| **3. GPU ç«¯** | GPU æ ¸å¿ƒ | Compute Units å¹¶è¡Œæ‰§è¡Œè®¡ç®— |
| **4. è¿”å›** | æ€»çº¿ä¼ è¾“ | ç»“æœä»æ˜¾å­˜æ¬å›ç³»ç»Ÿå†…å­˜ |

---

### 2.1.3 ç¼–è¯‘å™¨è§†è§’ï¼šROCm å¦‚ä½•ç”¨ LLVM/Clang æŠŠé«˜å±‚ä»£ç "é™ç»´"

> âš ï¸ **é‡è¦è®¤çŸ¥**ï¼šä½ å†™çš„ Python/HIP ä»£ç ï¼ŒGPU æ˜¯çœ‹ä¸æ‡‚çš„ã€‚ç¼–è¯‘å™¨éœ€è¦åšä¸€ç³»åˆ—è½¬æ¢æ‰èƒ½è®© GPU æ‰§è¡Œã€‚

#### ğŸ“‹ ç¼–è¯‘æµç¨‹

```mermaid
graph LR
    A[1. HIP æºç <br/>.hip/.cpp] --> B[2. Clang å‰ç«¯è§£æ<br/>è¯æ³•/è¯­æ³•åˆ†æ<br/>ç”Ÿæˆ LLVM IR]
    B --> C[3. LLVM ä¼˜åŒ–<br/>æ­»ä»£ç æ¶ˆé™¤ã€å¾ªç¯å±•å¼€ã€å‘é‡åŒ–]
    C --> D[4. LLVM åç«¯ AMDGPU<br/>è½¬æ¢ä¸º AMDGPU ISA<br/>å¯„å­˜å™¨åˆ†é…ã€æŒ‡ä»¤è°ƒåº¦]
    D --> E[5. Code Object ç”Ÿæˆ<br/>äºŒè¿›åˆ¶æ ¼å¼ .hsaco<br/>åŒ…å« ISA ä»£ç å’Œå…ƒæ•°æ®]

    style A fill:#e3f2fd
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#e8f5e9
    style E fill:#c8e6c9
```

#### ğŸ”¬ ç¤ºä¾‹ï¼šä¸€ä¸ªç®€å•çš„ HIP å‡½æ•°å¦‚ä½•è¢«ç¼–è¯‘
> ğŸ¯ **å®æˆ˜æ¼”ç»ƒ**ï¼šè®©æˆ‘ä»¬ç”¨å®é™…çš„ç¼–è¯‘å‘½ä»¤è¾“å‡º LLVM IR å’Œ ISAï¼

åˆ›å»ºæ–‡ä»¶ `simple_add.cpp`ï¼š

```cpp
// file: simple_add.cpp
#include <hip/hip_runtime.h>
#include <iostream>

__global__ void add(float* a, float* b, float* c, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        c[i] = a[i] + b[i];
    }
}

int main() {
    int n = 1024;
    size_t bytes = n * sizeof(float);

    float *a, *b, *c;
    hipMalloc(&a, bytes);
    hipMalloc(&b, bytes);
    hipMalloc(&c, bytes);

    // åˆå§‹åŒ–æ•°æ®
    float *h_a = new float[n];
    float *h_b = new float[n];
    for(int i = 0; i < n; i++) {
        h_a[i] = 1.0f;
        h_b[i] = 2.0f;
    }

    hipMemcpy(a, h_a, bytes, hipMemcpyHostToDevice);
    hipMemcpy(b, h_b, bytes, hipMemcpyHostToDevice);

    // å¯åŠ¨æ ¸å‡½æ•°
    hipLaunchKernelGGL(add, dim3(1), dim3(n), 0, 0, a, b, c, n);
    hipDeviceSynchronize();

    // éªŒè¯ç»“æœ
    float *h_c = new float[n];
    hipMemcpy(h_c, c, bytes, hipMemcpyDeviceToHost);

    std::cout << "Result: " << h_c[0] << ", " << h_c[n-1] << std::endl;

    delete[] h_a;
    delete[] h_b;
    delete[] h_c;
    hipFree(a);
    hipFree(b);
    hipFree(c);

    return 0;
}
```

**æ–¹æ³• 1ï¼šä½¿ç”¨ hipcc ç›´æ¥è¾“å‡º LLVM IR**

```bash
# è¾“å‡ºæœªç»ä¼˜åŒ–çš„ LLVM IR
hipcc --offload-arch=gfx1151 \
      -emit-llvm \
      -S \
      -O0 \
      simple_add.cpp -o simple_add_O0.ll

# è¾“å‡ºä¼˜åŒ–åçš„ LLVM IR
hipcc --offload-arch=gfx1151 \
      -emit-llvm \
      -S \
      -O3 \
      simple_add.cpp -o simple_add_O3.ll

# æŸ¥çœ‹ç”Ÿæˆçš„ LLVM IRï¼ˆåªæ˜¾ç¤º GPU kernel éƒ¨åˆ†ï¼‰
sed -n '/__CLANG_OFFLOAD_BUNDLE____START__ hip-amdgcn/,/__CLANG_OFFLOAD_BUNDLE____END__ hip-amdgcn/p' simple_add_O0.ll | grep -A 40 "define protected amdgpu_kernel"
sed -n '/__CLANG_OFFLOAD_BUNDLE____START__ hip-amdgcn/,/__CLANG_OFFLOAD_BUNDLE____END__ hip-amdgcn/p' simple_add_O3.ll | grep -A 40 "define protected amdgpu_kernel"
```

**å®é™…è¾“å‡ºç¤ºä¾‹**ï¼ˆæœªç»ä¼˜åŒ–çš„ LLVM IR -O0ï¼‰ï¼š

```llvm
; ç”Ÿæˆçš„æ–‡ä»¶: simple_add_O0.ll
define protected amdgpu_kernel void @_Z3addPfS_S_i(ptr addrspace(1) noundef %0, ptr addrspace(1) noundef %1, ptr addrspace(1) noundef %2, i32 noundef %3) #4 {
  %5 = alloca i32, align 4, addrspace(5)
  %6 = alloca i32, align 4, addrspace(5)
  %7 = alloca i32, align 4, addrspace(5)
  %8 = alloca i32, align 4, addrspace(5)
  %9 = alloca i32, align 4, addrspace(5)
  %10 = alloca i32, align 4, addrspace(5)
  %11 = alloca ptr, align 8, addrspace(5)
  %12 = alloca ptr, align 8, addrspace(5)
  %13 = alloca ptr, align 8, addrspace(5)
  %14 = alloca ptr, align 8, addrspace(5)
  %15 = alloca ptr, align 8, addrspace(5)
  %16 = alloca ptr, align 8, addrspace(5)
  %17 = alloca i32, align 4, addrspace(5)
  %18 = alloca i32, align 4, addrspace(5)
  %19 = addrspacecast ptr addrspace(5) %11 to ptr
  %20 = addrspacecast ptr addrspace(5) %12 to ptr
  %21 = addrspacecast ptr addrspace(5) %13 to ptr
  %22 = addrspacecast ptr addrspace(5) %14 to ptr
  %23 = addrspacecast ptr addrspace(5) %15 to ptr
  %24 = addrspacecast ptr addrspace(5) %16 to ptr
  %25 = addrspacecast ptr addrspace(5) %17 to ptr
  %26 = addrspacecast ptr addrspace(5) %18 to ptr
  store ptr addrspace(1) %0, ptr %19, align 8
  %27 = load ptr, ptr %19, align 8
  store ptr addrspace(1) %1, ptr %20, align 8
  %28 = load ptr, ptr %20, align 8
  store ptr addrspace(1) %2, ptr %21, align 8
  %29 = load ptr, ptr %21, align 8
  store ptr %27, ptr %22, align 8
  store ptr %28, ptr %23, align 8
  store ptr %29, ptr %24, align 8
  store i32 %3, ptr %25, align 4
  %30 = addrspacecast ptr addrspace(5) %10 to ptr
  %31 = addrspacecast ptr addrspace(5) %7 to ptr
  %32 = call i64 @__ockl_get_group_id(i32 noundef 0) #17
  %33 = trunc i64 %32 to i32
  %34 = addrspacecast ptr addrspace(5) %9 to ptr
  %35 = addrspacecast ptr addrspace(5) %6 to ptr
  %36 = call i64 @__ockl_get_local_size(i32 noundef 0) #17
  %37 = trunc i64 %36 to i32
  %38 = mul i32 %33, %37
  %39 = addrspacecast ptr addrspace(5) %8 to ptr
  %40 = addrspacecast ptr addrspace(5) %5 to ptr
  %41 = call i64 @__ockl_get_local_id(i32 noundef 0) #17
  %42 = trunc i64 %41 to i32
  %43 = add i32 %38, %42
  store i32 %43, ptr %26, align 4
  %44 = load i32, ptr %26, align 4
  %45 = load i32, ptr %25, align 4
  %46 = icmp slt i32 %44, %45
  br i1 %46, label %47, label %63

47:                                               ; preds = %4
  %48 = load ptr, ptr %22, align 8
  %49 = load i32, ptr %26, align 4
  %50 = sext i32 %49 to i64
  %51 = getelementptr inbounds float, ptr %48, i64 %50
  %52 = load float, ptr %51, align 4
  %53 = load ptr, ptr %23, align 8
  %54 = load i32, ptr %26, align 4
  %55 = sext i32 %54 to i64
  %56 = getelementptr inbounds float, ptr %53, i64 %55
  %57 = load float, ptr %56, align 4
  %58 = fadd contract float %52, %57
  %59 = load ptr, ptr %24, align 8
  %60 = load i32, ptr %26, align 4
  %61 = sext i32 %60 to i64
  %62 = getelementptr inbounds float, ptr %59, i64 %61
  store float %58, ptr %62, align 4
  br label %63

63:                                               ; preds = %47, %4
  ret void
}
; O0 ç‰ˆæœ¬åªæœ‰çº¦ 75 è¡Œä»£ç 
```

**å®é™…è¾“å‡ºç¤ºä¾‹**ï¼ˆä¼˜åŒ–åçš„ LLVM IR -O3ï¼‰ï¼š

```llvm
; ç”Ÿæˆçš„æ–‡ä»¶: simple_add_O3.ll
define protected amdgpu_kernel void @_Z3addPfS_S_i(ptr addrspace(1) noundef readonly captures(none) %0, ...) local_unnamed_addr #0 {
  ; æ— æ ˆåˆ†é…ï¼æ‰€æœ‰å˜é‡éƒ½åœ¨å¯„å­˜å™¨ä¸­

  ; ç›´æ¥è°ƒç”¨ LLVM intrinsicï¼ˆæ— ä¸­é—´å±‚ï¼‰
  %5 = tail call i32 @llvm.amdgcn.workgroup.id.x()
  %6 = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
  %7 = getelementptr inbounds nuw i8, ptr addrspace(4) %6, i64 12
  %8 = load i16, ptr addrspace(4) %7, align 4, !tbaa !6
  %9 = zext i16 %8 to i32
  %10 = mul i32 %5, %9
  %11 = tail call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.x()
  %12 = add i32 %10, %11
  %13 = icmp slt i32 %12, %3
  br i1 %13, label %14, label %22

14:                                               ; preds = %4
  %15 = sext i32 %12 to i64
  %16 = getelementptr inbounds float, ptr addrspace(1) %2, i64 %15
  %17 = getelementptr inbounds float, ptr addrspace(1) %1, i64 %15
  %18 = getelementptr inbounds float, ptr addrspace(1) %0, i64 %15
  %19 = load float, ptr addrspace(1) %18, align 4, !tbaa !10
  %20 = load float, ptr addrspace(1) %17, align 4, !tbaa !10
  %21 = fadd contract float %19, %20
  store float %21, ptr addrspace(1) %16, align 4, !tbaa !10
  br label %22

22:                                               ; preds = %14, %4
  ret void
}
; O3 ç‰ˆæœ¬åªæœ‰çº¦ 35 è¡Œä»£ç 
```

<div style="background: #e3f2fd; border: 1px solid #2196f3; border-radius: 8px; padding: 16px; margin: 16px 0;">
  <div style="display: flex; align-items: start;">
    <span style="font-size: 20px; margin-right: 10px;">ğŸ”</span>
    <div>
      <strong style="color: #1565c0;">O0 vs O3 å…³é”®å·®å¼‚å¯¹æ¯”</strong><br>
      <span style="color: #1565c0; line-height: 1.6;">
        <strong>æ ˆä½¿ç”¨ï¼š</strong> O0 ä½¿ç”¨ 18 ä¸ª allocaï¼ˆaddrspace(5) private memoryï¼‰ï¼ŒO3 å®Œå…¨æ— æ ˆåˆ†é…<br/>
        <strong>å‡½æ•°è°ƒç”¨ï¼š</strong> O0 ä½¿ç”¨ <code>@__ockl_get_group_id</code> ç­‰åŒ…è£…å‡½æ•°ï¼ŒO3 ç›´æ¥è°ƒç”¨ <code>@llvm.amdgcn.workgroup.id.x()</code> intrinsic<br/>
        <strong>ä»£ç é‡ï¼š</strong> O0 çº¦ 75 è¡Œï¼ŒO3 ä»… 35 è¡Œï¼ˆä¼˜åŒ–æ‰ 50%ï¼‰<br/>
        <strong>å†…å­˜è®¿é—®ï¼š</strong> O0 å¤šæ¬¡å†—ä½™ load/storeï¼ŒO3 æ‰€æœ‰å˜é‡ä¿æŒå¯„å­˜å™¨<br/>
        <strong>TBAA æ³¨è§£ï¼š</strong> O3 æ·»åŠ  <code>!tbaa !6</code> å’Œ <code>!tbaa !10</code> ç”¨äºç±»å‹åˆ«ååˆ†æä¼˜åŒ–<br/>
        <strong>å‚æ•°å±æ€§ï¼š</strong> O3 æ·»åŠ  <code>readonly</code>/<code>writeonly</code>/<code>captures(none)</code> å¸®åŠ©ç¼–è¯‘å™¨ä¼˜åŒ–
      </span>
    </div>
  </div>
</div>


**ï¼ˆå¯é€‰ï¼‰è¾“å‡º AMDGPU ISA æ±‡ç¼–ä»£ç **ï¼š

```bash
# è¾“å‡º ISA æ±‡ç¼–ä»£ç 
hipcc --offload-arch=gfx1151 -S -O3 simple_add.cpp -o simple_add.s

# æŸ¥çœ‹ç”Ÿæˆçš„æ±‡ç¼–ï¼ˆåªæ˜¾ç¤º GPU kernel éƒ¨åˆ†ï¼‰
sed -n '/__CLANG_OFFLOAD_BUNDLE____START__ hip-amdgcn/,/__CLANG_OFFLOAD_BUNDLE____END__ hip-amdgcn/p' simple_add.s | head -100
```

**å®é™…è¾“å‡ºç¤ºä¾‹**ï¼ˆAMDGPU ISA æ±‡ç¼–ï¼‰ï¼š

```asm
# __CLANG_OFFLOAD_BUNDLE____START__ hip-amdgcn-amd-amdhsa--gfx1151
        .amdgcn_target "amdgcn-amd-amdhsa--gfx1151"
        .amdhsa_code_object_version 6
        .text
        .protected      _Z3addPfS_S_i           ; -- Begin function _Z3addPfS_S_i
        .globl  _Z3addPfS_S_i
        .p2align        8
        .type   _Z3addPfS_S_i,@function
_Z3addPfS_S_i:                          ; @_Z3addPfS_S_i
; %bb.0:
        s_clause 0x1
        s_load_b32 s3, s[0:1], 0x2c
        s_load_b32 s4, s[0:1], 0x18
        s_waitcnt lgkmcnt(0)
        s_and_b32 s3, s3, 0xffff
        s_delay_alu instid0(SALU_CYCLE_1)
        v_mad_u64_u32 v[0:1], null, s2, s3, v[0:1]
        s_mov_b32 s2, exec_lo
        v_cmpx_gt_i32_e64 s4, v0
        s_cbranch_execz .LBB0_2
; %bb.1:
        s_load_b128 s[4:7], s[0:1], 0x0
        v_ashrrev_i32_e32 v1, 31, v0
        s_load_b64 s[0:1], s[0:1], 0x10
        s_delay_alu instid0(VALU_DEP_1) | instskip(SKIP_1) | instid1(VALU_DEP_1)
        v_lshlrev_b64 v[0:1], 2, v[0:1]
        s_waitcnt lgkmcnt(0)
        v_add_co_u32 v2, vcc_lo, s4, v0
        s_delay_alu instid0(VALU_DEP_1) | instskip(SKIP_1) | instid1(VALU_DEP_1)
        v_add_co_ci_u32_e64 v3, null, s5, v1, vcc_lo
        v_add_co_u32 v4, vcc_lo, s6, v0
        v_add_co_ci_u32_e64 v5, null, s7, v1, vcc_lo
        global_load_b32 v2, v[2:3], off
        global_load_b32 v3, v[4:5], off
        v_add_co_u32 v0, vcc_lo, s0, v0
        s_delay_alu instid0(VALU_DEP_1)
        v_add_co_ci_u32_e64 v1, null, s1, v1, vcc_lo
        s_waitcnt vmcnt(0)
        v_add_f32_e32 v2, v2, v3
        global_store_b32 v[0:1], v2, off
.LBB0_2:
        s_endpgm
        .section        .rodata,"a",@progbits
        .p2align        6, 0x0
        .amdhsa_kernel _Z3addPfS_S_i
                .amdhsa_group_segment_fixed_size 0
                .amdhsa_private_segment_fixed_size 0
                .amdhsa_kernarg_size 288
                .amdhsa_user_sgpr_count 2
                .amdhsa_user_sgpr_dispatch_ptr 0
                .amdhsa_user_sgpr_queue_ptr 0
                .amdhsa_user_sgpr_kernarg_segment_ptr 1
                .amdhsa_user_sgpr_dispatch_id 0
                .amdhsa_user_sgpr_private_segment_size 0
                .amdhsa_wavefront_size32 1
                .amdhsa_uses_dynamic_stack 0
                .amdhsa_enable_private_segment 0
                .amdhsa_system_sgpr_workgroup_id_x 1
                .amdhsa_system_sgpr_workgroup_id_y 0
                .amdhsa_system_sgpr_workgroup_id_z 0
                .amdhsa_system_sgpr_workgroup_info 0
                .amdhsa_system_vgpr_workitem_id 0
                .amdhsa_next_free_vgpr 6
                .amdhsa_next_free_sgpr 8
                .amdhsa_reserve_vcc 1
                .amdhsa_float_round_mode_32 0
                .amdhsa_float_round_mode_16_64 0
                .amdhsa_float_denorm_mode_32 3
                .amdhsa_float_denorm_mode_16_64 3
                .amdhsa_dx10_clamp 1
                .amdhsa_ieee_mode 1
                .amdhsa_fp16_overflow 0
                .amdhsa_workgroup_processor_mode 1
                .amdhsa_memory_ordered 1
                .amdhsa_forward_progress 1
                .amdhsa_shared_vgpr_count 0
                .amdhsa_inst_pref_size 2
                .amdhsa_exception_fp_ieee_invalid_op 0
                .amdhsa_exception_fp_denorm_src 0
                .amdhsa_exception_fp_ieee_div_zero 0
                .amdhsa_exception_fp_ieee_overflow 0
                .amdhsa_exception_fp_ieee_underflow 0
                .amdhsa_exception_fp_ieee_inexact 0
                .amdhsa_exception_int_div_zero 0
        .end_amdhsa_kernel
        .text
.Lfunc_end0:
        .size   _Z3addPfS_S_i, .Lfunc_end0-_Z3addPfS_S_i
                                        ; -- End function
        .set _Z3addPfS_S_i.num_vgpr, 6
        .set _Z3addPfS_S_i.num_agpr, 0
        .set _Z3addPfS_S_i.numbered_sgpr, 8
        .set _Z3addPfS_S_i.num_named_barrier, 0
        .set _Z3addPfS_S_i.private_seg_size, 0
        .set _Z3addPfS_S_i.uses_vcc, 1
        .set _Z3addPfS_S_i.uses_flat_scratch, 0
        .set _Z3addPfS_S_i.has_dyn_sized_stack, 0
        .set _Z3addPfS_S_i.has_recursion, 0
        .set _Z3addPfS_S_i.has_indirect_call, 0
        .section        .AMDGPU.csdata,"",@progbits
```

**å…³é”®æ±‡ç¼–æŒ‡ä»¤è§£è¯»**ï¼š

| æŒ‡ä»¤ | è¯´æ˜ |
|:---|:---|
| `s_load_b32` | æ ‡é‡åŠ è½½ï¼šä»å¸¸é‡å†…å­˜åŠ è½½åˆ° SGPR |
| `v_mad_u64_u32` | å‘é‡ä¹˜åŠ ï¼šè®¡ç®—çº¿ç¨‹å…¨å±€ ID |
| `v_cmpx_gt_i32` | å‘é‡æ¯”è¾ƒï¼šè¾¹ç•Œæ£€æŸ¥ï¼ŒåŒæ—¶æ›´æ–°æ‰§è¡Œæ©ç  |
| `global_load_b32` | å…¨å±€å†…å­˜åŠ è½½ï¼šä»æ˜¾å­˜è¯»å–æ•°æ® |
| `v_add_f32_e32` | å‘é‡æµ®ç‚¹åŠ æ³•ï¼šæ‰§è¡Œå®é™…çš„åŠ æ³•è¿ç®— |
| `global_store_b32` | å…¨å±€å†…å­˜å­˜å‚¨ï¼šå†™å›æ˜¾å­˜ |
| `s_endpgm` | ç¨‹åºç»“æŸï¼šç»ˆæ­¢ kernel æ‰§è¡Œ |


#### ğŸ’¡ ä¸ºä»€ä¹ˆéœ€è¦è¿è¡Œæ—¶ç¼–è¯‘ï¼ˆJITï¼‰ï¼Ÿ

PyTorch æœ‰ä¸€ä¸ªå¼ºå¤§çš„èƒ½åŠ›ï¼š**è¿è¡Œæ—¶ç¼–è¯‘**ã€‚å½“ä½ å†™ä¸€ä¸ªè‡ªå®šä¹‰ç®—å­æ—¶ï¼ŒPyTorch ä¼šï¼š

1. åœ¨è¿è¡Œæ—¶è°ƒç”¨ `hiprtc` (HIP Runtime Compilation)
2. ä½¿ç”¨ `libamd_comgr` ç¼–è¯‘ä½ çš„ HIP ä»£ç 
3. ç”Ÿæˆé€‚é…å½“å‰ GPU æ¶æ„çš„äºŒè¿›åˆ¶
4. åŠ è½½åˆ° GPU æ‰§è¡Œ

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ PyTorch èƒ½"åŠ¨æ€ç¼–è¯‘" HIP ç®—å­ã€‚

---

### 2.1.4 å®æˆ˜éªŒè¯ï¼šæ‰‹å†™ç¬¬ä¸€ä¸ª HIP ç¨‹åº

> ğŸ¯ **å®æˆ˜ç›®æ ‡**ï¼šç°åœ¨æˆ‘ä»¬è·³è¿‡ Pythonï¼Œç›´æ¥ç”¨ C++ å†™ä¸€ä¸ª HIP ç¨‹åºï¼ŒéªŒè¯æ•´ä¸ªè°ƒç”¨é“¾è·¯ã€‚

#### ğŸ“ åˆ›å»ºæ–‡ä»¶ `hello_rocm.cpp`

```cpp
#include <hip/hip_runtime.h>
#include <iostream>
#include <cstdlib>

// å®šä¹‰ä¸€ä¸ªå®æ¥æ£€æŸ¥ HIP API çš„è¿”å›å€¼
#define HIP_CHECK(call)                                                         \
    do {                                                                        \
        hipError_t err = call;                                                  \
        if (err != hipSuccess) {                                                \
            std::cerr << "HIP Error: " << hipGetErrorString(err)                \
                      << " at line " << __LINE__ << std::endl;                  \
            std::exit(EXIT_FAILURE);                                            \
        }                                                                       \
    } while (0)

// è¿™æ˜¯ä¸€ä¸ª"æ ¸å‡½æ•°"(Kernel)ï¼Œå®ƒå°†åœ¨ AMD GPU ä¸Šè¿è¡Œ
// __global__ æ˜¯å‘Šè¯‰ç¼–è¯‘å™¨ï¼šè¿™ä¸ªå‡½æ•°åœ¨ GPU ä¸Šè·‘ï¼Œä½†ç”± CPU è°ƒç”¨
__global__ void vector_add(float *a, float *b, float *c, int n) {
    // è·å–å½“å‰çº¿ç¨‹çš„ ID
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i < n) {
        c[i] = a[i] + b[i]; // æ¯ä¸ªçº¿ç¨‹åªç®—ä¸€ä¸ªæ•°çš„åŠ æ³•
    }
}

int main() {
    int n = 1024; // å‘é‡é•¿åº¦
    size_t bytes = n * sizeof(float);

    // 1. åœ¨ CPU (Host) ä¸Šåˆ†é…å†…å­˜
    float *h_a, *h_b, *h_c;
    h_a = (float*)malloc(bytes);
    h_b = (float*)malloc(bytes);
    h_c = (float*)malloc(bytes);

    // åˆå§‹åŒ–æ•°æ®
    for(int i=0; i<n; i++) {
        h_a[i] = 1.0f;
        h_b[i] = 2.0f;
    }

    // 2. åœ¨ GPU (Device) ä¸Šåˆ†é…æ˜¾å­˜
    float *d_a, *d_b, *d_c;
    HIP_CHECK(hipMalloc(&d_a, bytes));
    HIP_CHECK(hipMalloc(&d_b, bytes));
    HIP_CHECK(hipMalloc(&d_c, bytes));

    // 3. æŠŠæ•°æ®ä» CPU æ¬è¿åˆ° GPU (H2D)
    HIP_CHECK(hipMemcpy(d_a, h_a, bytes, hipMemcpyHostToDevice));
    HIP_CHECK(hipMemcpy(d_b, h_b, bytes, hipMemcpyHostToDevice));

    // 4. å¯åŠ¨æ ¸å‡½æ•°ï¼è®©æ˜¾å¡å¹²æ´»
    // è¯­æ³•: <<<GridDim, BlockDim>>>
    // è¿™é‡Œå¼€å¯ 1 ä¸ª Blockï¼Œé‡Œé¢æœ‰ 1024 ä¸ªçº¿ç¨‹å¹¶è¡Œè®¡ç®—
    hipLaunchKernelGGL(vector_add, dim3(1), dim3(n), 0, 0, d_a, d_b, d_c, n);

    // ç­‰å¾… GPU å¹²å®Œæ´»
    HIP_CHECK(hipDeviceSynchronize());

    // 5. æŠŠç»“æœæ¬å› CPU (D2H)
    HIP_CHECK(hipMemcpy(h_c, d_c, bytes, hipMemcpyDeviceToHost));

    // éªŒè¯ç»“æœ
    std::cout << "Element [0]: " << h_a[0] << " + " << h_b[0] << " = " << h_c[0] << std::endl;
    std::cout << "Element [1023]: " << h_a[1023] << " + " << h_b[1023] << " = " << h_c[1023] << std::endl;
    std::cout << ">>> ROCm HIP Kernel executed successfully on AMD GPU!" << std::endl;

    // æ¸…ç†å†…å­˜
    HIP_CHECK(hipFree(d_a)); HIP_CHECK(hipFree(d_b)); HIP_CHECK(hipFree(d_c));
    free(h_a); free(h_b); free(h_c);

    return 0;
}
```

#### ğŸš€ ç¼–è¯‘å¹¶è¿è¡Œ

```bash
# ç¡®è®¤ hipcc ç¼–è¯‘å™¨æ˜¯å¦å°±ç»ª
which hipcc

# ç¼–è¯‘
hipcc hello_rocm.cpp -o hello_rocm

# è¿è¡Œ
./hello_rocm
```

**é¢„æœŸè¾“å‡º**ï¼š
```
Element [0]: 1 + 2 = 3
Element [1023]: 1 + 2 = 3
>>> ROCm HIP Kernel executed successfully on AMD GPU!
```

#### ğŸ“Š ç¨‹åºæ‰§è¡Œæµç¨‹

| æ­¥éª¤ | CPU ç«¯ | GPU ç«¯ |
|:---|:---|:---|
| **1. åˆ†é…å†…å­˜** | `malloc` åˆ†é…ç³»ç»Ÿå†…å­˜ | `hipMalloc` åˆ†é…æ˜¾å­˜ |
| **2. æ•°æ®æ¬è¿** | `hipMemcpy(H2D)` æŠŠæ•°æ®æ¬åˆ°æ˜¾å­˜ | ç­‰å¾…æ•°æ® |
| **3. å¯åŠ¨è®¡ç®—** | `hipLaunchKernelGGL` å‘å°„è®¡ç®—ä»»åŠ¡ | 1024 ä¸ªçº¿ç¨‹å¹¶è¡Œè®¡ç®— |
| **4. åŒæ­¥** | `hipDeviceSynchronize` ç­‰å¾… GPU å®Œæˆ | å®Œæˆè®¡ç®— |
| **5. ç»“æœæ¬è¿** | `hipMemcpy(D2H)` æŠŠç»“æœæ¬å›å†…å­˜ | è¿”å›ç»“æœ |

<div style="background: #e8f5e9; border: 1px solid #4caf50; border-radius: 8px; padding: 16px; margin: 16px 0;">
  <div style="display: flex; align-items: start;">
    <span style="font-size: 20px; margin-right: 10px;">ğŸ‰</span>
    <div>
      <strong style="color: #2e7d32;">æ­å–œï¼ä½ åˆšåˆšå®Œæˆäº†ï¼š</strong><br>
      <span style="color: #2e7d32; line-height: 1.6;">
        â€¢ ç¬¬ä¸€æ¬¡æ‰‹åŠ¨ç®¡ç† GPU å†…å­˜<br/>
        â€¢ ç¬¬ä¸€æ¬¡æ‰‹åŠ¨å¯åŠ¨ GPU æ ¸å‡½æ•°<br/>
        â€¢ ç¬¬ä¸€æ¬¡å®Œæ•´èµ°é€šäº† PyTorch åº•å±‚çš„æ•´ä¸ªè°ƒç”¨é“¾è·¯
      </span>
    </div>
  </div>
</div>

---

## 2.2 ç¡¬ä»¶æ€ç»´é©å‘½ï¼šä» CPU åˆ° GPU çš„èŒƒå¼è½¬å˜

ç°åœ¨ä½ å·²ç»çŸ¥é“ä»£ç å¦‚ä½•è·‘åˆ° GPU ä¸Šäº†ï¼Œä½†ä¸€ä¸ªæ›´æ ¹æœ¬çš„é—®é¢˜æ˜¯ï¼š**ä¸ºä»€ä¹ˆ AI å¿…é¡»ç”¨ GPUï¼Ÿä¸ºä»€ä¹ˆ CPU ç®—ä¸åŠ¨ï¼Ÿ**

ç­”æ¡ˆåœ¨äº CPU å’Œ GPUçš„**è®¾è®¡å“²å­¦å®Œå…¨ä¸åŒ**ã€‚

### 2.2.1 ä¸ºä»€ä¹ˆ CPU ç®—ä¸åŠ¨ AIï¼Ÿ

#### ğŸ§  CPU çš„è®¾è®¡å“²å­¦ï¼šä½å»¶è¿Ÿ

CPU æ˜¯ä¸º**é€šç”¨è®¡ç®—**è®¾è®¡çš„ï¼Œå®ƒçš„è®¾è®¡ç›®æ ‡æ˜¯ï¼š

| è®¾è®¡ç›®æ ‡ | è¯´æ˜ |
|:---|:---|
| **ä½å»¶è¿Ÿ** | è®©å•ä¸ªä»»åŠ¡å°½å¯èƒ½å¿«åœ°å®Œæˆ |
| **å¤æ‚æ§åˆ¶æµ** | æ”¯æŒå¤æ‚çš„åˆ†æ”¯é¢„æµ‹ã€ä¹±åºæ‰§è¡Œ |
| **å¤§ç¼“å­˜** | L1/L2/L3 ç¼“å­˜å‡å°‘å†…å­˜è®¿é—®å»¶è¿Ÿ |
| **å°‘é‡å¼ºå¤§çš„æ ¸å¿ƒ** | é€šå¸¸ 4-128 ä¸ªæ ¸å¿ƒï¼Œæ¯ä¸ªæ ¸å¿ƒå¾ˆå¼º |

**CPU æ“…é•¿çš„ä»»åŠ¡**ï¼š
- âœ… æ“ä½œç³»ç»Ÿè°ƒåº¦
- âœ… æ•°æ®åº“æŸ¥è¯¢
- âœ… é€»è¾‘å¤æ‚çš„ä¸šåŠ¡ä»£ç 
- âœ… åˆ†æ”¯å¾ˆå¤šçš„ç®—æ³•

#### âš¡ AI è®¡ç®—çš„ç‰¹ç‚¹ï¼šé«˜åå

AIï¼ˆæ·±åº¦å­¦ä¹ ï¼‰çš„è´Ÿè½½å®Œå…¨ä¸åŒï¼š

| è®¡ç®—ç‰¹ç‚¹ | è¯´æ˜ |
|:---|:---|
| **æ•°æ®å¹¶è¡Œ** | åŒæ—¶å¤„ç†æˆåƒä¸Šä¸‡ä¸ªæ•°æ® |
| **è§„åˆ™ç®€å•** | ä¸»è¦æ˜¯çŸ©é˜µä¹˜æ³•ã€å·ç§¯ |
| **è®¡ç®—å¯†é›†** | æ¯ä¸ªæ•°æ®éœ€è¦å¤§é‡æµ®ç‚¹è¿ç®— |
| **å†…å­˜å¸¦å®½æ•æ„Ÿ** | éœ€è¦å¿«é€Ÿæ¬è¿å¤§é‡æ•°æ® |

![CPU vs GPUï¼šä½å»¶è¿Ÿvs é«˜åå](images/01_cpu_gpu_throughput.svg)

#### âš–ï¸ CPU vs AI éœ€æ±‚çš„ä¸åŒ¹é…

| CPU çš„ä¼˜åŒ– | AI çš„éœ€æ±‚ | ç»“æœ |
|:---|:---|:---|
| å¤§ç¼“å­˜å‡å°‘å»¶è¿Ÿ | éœ€è¦é«˜å¸¦å®½æ¬æ•°æ® | âŒ ç¼“å­˜å¤ªå°ï¼Œè£…ä¸ä¸‹æ¨¡å‹ |
| å°‘é‡å¼ºå¤§æ ¸å¿ƒ | éœ€è¦æ•°åƒå¼±æ ¸å¿ƒå¹¶è¡Œ | âŒ å¹¶è¡Œåº¦ä¸å¤Ÿ |
| å¤æ‚åˆ†æ”¯é¢„æµ‹ | ç®€å•é‡å¤è®¡ç®— | âŒ åˆ†æ”¯é¢„æµ‹å™¨æµªè´¹èµ„æº |

#### ğŸ“Š æ€§èƒ½å¯¹æ¯”ï¼šResNet-50 æ¨ç†

æˆ‘ä»¬ç”¨ä¸€ä¸ªå®é™…ä¾‹å­è¯´æ˜ï¼šåœ¨ CPU vs GPU ä¸Šè¿è¡Œ ResNet-50 æ¨ç†ã€‚

> ğŸ¯ **å®æˆ˜æ¼”ç»ƒ**ï¼šè®©æˆ‘ä»¬åœ¨ Radeon 8060S (gfx1151) ä¸Šå®é™…æµ‹è¯•ä¸€ä¸‹ï¼

åˆ›å»ºæ–‡ä»¶ `bench_resnet.py`ï¼š

```python
# file: bench_resnet.py
import torch
import torchvision
import time

# è®¾ç½®è®¾å¤‡
device_cpu = torch.device("cpu")
device_gpu = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"=== GPU ä¿¡æ¯ ===")
if torch.cuda.is_available():
    print(f"GPU è®¾å¤‡: {torch.cuda.get_device_name(0)}")
    props = torch.cuda.get_device_properties(0)
    print(f"æ¶æ„: {props.name}")  # åº”è¯¥æ˜¾ç¤º gfx1151
    print(f"æ˜¾å­˜: {props.total_memory / 1024**3:.1f} GB")
    print(f"è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
else:
    print("æœªæ£€æµ‹åˆ° GPU")

# åŠ è½½ ResNet-50 æ¨¡å‹
print("\n=== åŠ è½½ ResNet-50 æ¨¡å‹ ===")
model = torchvision.models.resnet50(pretrained=True)
model.eval()

# å‡†å¤‡æµ‹è¯•æ•°æ®
batch_size = 32
dummy_input = torch.randn(batch_size, 3, 224, 224)

# ========== CPU æ¨ç†æµ‹è¯• ==========
print("\n=== CPU æ¨ç†æµ‹è¯• ===")
model_cpu = model.to(device_cpu)

# é¢„çƒ­
for _ in range(3):
    with torch.no_grad():
        _ = model_cpu(dummy_input)

# æ­£å¼æµ‹è¯•
torch.cpu.synchronize() if hasattr(torch, 'cpu') else None
start = time.time()
num_iterations = 10
with torch.no_grad():
    for _ in range(num_iterations):
        _ = model_cpu(dummy_input)
end = time.time()

cpu_time_ms = (end - start) / num_iterations * 1000 / batch_size
cpu_throughput = batch_size * num_iterations / (end - start)

print(f"CPU å¹³å‡å»¶è¿Ÿ: {cpu_time_ms:.2f} ms/image")
print(f"CPU ååé‡: {cpu_throughput:.2f} img/s")

# ========== GPU æ¨ç†æµ‹è¯• ==========
if torch.cuda.is_available():
    print("\n=== GPU æ¨ç†æµ‹è¯• ===")
    model_gpu = model.to(device_gpu)
    dummy_input_gpu = dummy_input.to(device_gpu)

    # é¢„çƒ­
    for _ in range(10):
        with torch.no_grad():
            _ = model_gpu(dummy_input_gpu)
        torch.cuda.synchronize()

    # æ­£å¼æµ‹è¯•
    torch.cuda.synchronize()
    start = time.time()
    num_iterations = 100
    with torch.no_grad():
        for _ in range(num_iterations):
            _ = model_gpu(dummy_input_gpu)
    torch.cuda.synchronize()
    end = time.time()

    gpu_time_ms = (end - start) / num_iterations * 1000 / batch_size
    gpu_throughput = batch_size * num_iterations / (end - start)

    print(f"GPU å¹³å‡å»¶è¿Ÿ: {gpu_time_ms:.2f} ms/image")
    print(f"GPU ååé‡: {gpu_throughput:.2f} img/s")

    # è®¡ç®—åŠ é€Ÿæ¯”
    speedup = cpu_time_ms / gpu_time_ms
    print(f"\nğŸš€ GPU åŠ é€Ÿæ¯”: {speedup:.1f}x")

    # æ˜¾å­˜ä½¿ç”¨
    print(f"\næ˜¾å­˜ä½¿ç”¨: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
```

**è¿è¡Œæµ‹è¯•**ï¼š

```bash
python bench_resnet.py
```

**é¢„æœŸè¾“å‡ºç¤ºä¾‹**ï¼ˆåœ¨ Radeon 8060S ä¸Šï¼‰ï¼š
![alt text](images/2.bench_resnet.png)


<div style="background: #fff3e0; border: 1px solid #ff9800; border-radius: 8px; padding: 16px; margin: 16px 0;">
  <div style="display: flex; align-items: start;">
    <span style="font-size: 20px; margin-right: 10px;">ğŸš€</span>
    <div>
      <strong style="color: #ef6c00;">GPU vs CPU æ€§èƒ½å¯¹æ¯”åˆ†æ</strong><br>
      <span style="color: #ef6c00; line-height: 1.6;">
        ä»æµ‹è¯•ç»“æœå¯ä»¥çœ‹åˆ°ï¼š<br/>
        â€¢ Radeon 8060S (gfx1151) æ˜¯æ¶ˆè´¹çº§æ˜¾å¡ï¼Œä»ç„¶æœ‰æ•°åä¸ª CU<br/>
        â€¢ æ¯ä¸ª CU èƒ½åŒæ—¶è·‘å¤šä¸ª wavefrontï¼ˆæ¯ä¸ª wavefront æ˜¯ 32/64 ä¸ªçº¿ç¨‹ï¼‰<br/>
        â€¢ å¯¹æ¯” CPUï¼šå³ä½¿å¤šæ ¸ CPUï¼Œå¹¶è¡Œåº¦ä»ç„¶è¿œä½äº GPU<br/>
        â€¢ <strong>å·®è·çš„æ ¸å¿ƒ</strong>ï¼šGPU ç”¨"äººæµ·æˆ˜æœ¯"ï¼ŒCPU æ˜¯"ç²¾è‹±æˆ˜æœ¯"
      </span>
    </div>
  </div>
</div>

---

### 2.2.2 SIMT æ¨¡å‹å›¾è§£ï¼šå•æŒ‡ä»¤å¤šçº¿ç¨‹çš„é­”åŠ›

GPU çš„æ ¸å¿ƒæŠ€æœ¯æ˜¯ **SIMT (Single Instruction, Multiple Threads)**ï¼Œå³"å•æŒ‡ä»¤å¤šçº¿ç¨‹"ã€‚

![æ§åˆ¶å•å…ƒvs è¿ç®—å•å…ƒï¼šCPU ä¸ GPU çš„æ¯”ä¾‹å·®å¼‚](images/02_control_alu.svg)

#### ğŸ”‘ å…³é”®æ¦‚å¿µ

| æ¦‚å¿µ | NVIDIA | AMD | è¯´æ˜ |
|:---|:---|:---|:---|
| **çº¿ç¨‹ç»„** | Warp (32çº¿ç¨‹) | **Wavefront (32/64çº¿ç¨‹)** | ä¸€ç»„çº¿ç¨‹ä¸€èµ·æ‰§è¡Œç›¸åŒæŒ‡ä»¤ |
| **åˆ†æ”¯å‘æ•£** | Warp Divergence | Wavefront Divergence | å¦‚æœæœ‰ if-else åˆ†æ”¯ï¼Œæ€§èƒ½ä¼šä¸‹é™ |

> ğŸ’¡ **ä¸Šå›¾è¯´æ˜äº†ä»€ä¹ˆï¼Ÿ**
> - **CPU**ï¼šä»»åŠ¡æŒ‰é¡ºåºä¸€ä¸ªæ¥ä¸€ä¸ªæ‰§è¡Œï¼Œå°±åƒå•äººæ’é˜Ÿæ‰“é¥­
> - **GPU**ï¼š32ä¸ªçº¿ç¨‹ï¼ˆä¸€ä¸ª Wavefrontï¼‰åŒæ—¶æ‰§è¡Œç›¸åŒæŒ‡ä»¤ï¼Œå°±åƒä¸€ä¸ªç­çš„åŒå­¦ä¸€èµ·åšæ“

#### âš ï¸ åˆ†æ”¯å‘æ•£é—®é¢˜

å½“ wavefront å†…çš„çº¿ç¨‹éœ€è¦æ‰§è¡Œä¸åŒçš„ä»£ç è·¯å¾„æ—¶ï¼Œå°±ä¼šå‘ç”Ÿ**åˆ†æ”¯å‘æ•£**ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

**ç³Ÿç³•çš„ä»£ç **ï¼š
```cpp
__global__ void bad_branch(float* data) {
    int i = threadIdx.x;
    if (i % 2 == 0) {  // åˆ†æ”¯å‘æ•£ï¼
        data[i] *= 2.0f;
    } else {
        data[i] += 1.0f;
    }
}
```

**æ‰§è¡Œæƒ…å†µ**ï¼š
- Wavefront åŒ…å«çº¿ç¨‹ 0-31
- çº¿ç¨‹ 0,2,4,... æ‰§è¡Œ `if` åˆ†æ”¯
- çº¿ç¨‹ 1,3,5,... æ‰§è¡Œ `else` åˆ†æ”¯
- **GPU ä¸å¾—ä¸ä¸²è¡Œæ‰§è¡Œä¸¤ä¸ªåˆ†æ”¯**ï¼Œæ€§èƒ½å‡åŠï¼

**å¥½çš„ä»£ç **ï¼š
```cpp
__global__ void good_branch(float* data) {
    int i = threadIdx.x;
    data[i] = data[i] * 2.0f + 1.0f;  // æ— åˆ†æ”¯ï¼Œå…¨éƒ¨å¹¶è¡Œ
}
```

> ğŸ¯ **å®æˆ˜æµ‹è¯•**ï¼šè®©æˆ‘ä»¬å®é™…æµ‹è¯•ä¸€ä¸‹åˆ†æ”¯å‘æ•£å¯¹æ€§èƒ½çš„å½±å“ï¼

åˆ›å»ºæ–‡ä»¶ `bench_divergence.cpp`ï¼š

```cpp
// file: bench_divergence.cpp
#include <hip/hip_runtime.h>
#include <iostream>
#include <vector>
#include <cstdlib>

#define HIP_CHECK(call) \
    do { \
        hipError_t err = call; \
        if (err != hipSuccess) { \
            std::cerr << "HIP Error: " << hipGetErrorString(err) \
                      << " at line " << __LINE__ << std::endl; \
            std::exit(EXIT_FAILURE); \
        } \
    } while (0)

constexpr int WARP_SIZE = 32;

// =============== ç‰ˆæœ¬1ï¼šåˆ†æ”¯+åŒæ­¥çš„ä¼ ç»Ÿ reduction ===============
__global__ void reduce_branchy(const float* __restrict__ in,
                               float* __restrict__ out,
                               int n) {
    extern __shared__ float sdata[];
    int tid = threadIdx.x;
    int global = blockIdx.x * blockDim.x * 2 + tid;

    float sum = 0.0f;
    if (global < n) sum += in[global];
    if (global + blockDim.x < n) sum += in[global + blockDim.x];
    sdata[tid] = sum;
    __syncthreads();

    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (tid < stride) {
            sdata[tid] += sdata[tid + stride];
        }
        __syncthreads();
    }

    if (tid == 0) out[blockIdx.x] = sdata[0];
}

// =============== wavefront shuffle sum ===============
__device__ __forceinline__ float wf_reduce_sum(float v) {
    // ç”¨å›ºå®š WARP_SIZEï¼Œé¿å… warpSize åœ¨ host/device æ··ç”¨çš„å‘
    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {
        v += __shfl_down(v, offset, WARP_SIZE);
    }
    return v;
}

__global__ void reduce_shuffle(const float* __restrict__ in,
                               float* __restrict__ out,
                               int n) {
    // æ¯ä¸ª wavefront ä¸€ä¸ª partial sum
    extern __shared__ float wf_sums[];

    int tid = threadIdx.x;
    int idx = blockIdx.x * blockDim.x * 2 + tid;

    float v = 0.0f;
    if (idx < n) v += in[idx];
    if (idx + blockDim.x < n) v += in[idx + blockDim.x];

    float wf_sum = wf_reduce_sum(v);

    int lane = tid % WARP_SIZE;
    int wid  = tid / WARP_SIZE;

    if (lane == 0) wf_sums[wid] = wf_sum;
    __syncthreads();

    // wave0 æ±‡æ€»æ‰€æœ‰ wave partials
    if (wid == 0) {
        int num_waves = (blockDim.x + WARP_SIZE - 1) / WARP_SIZE;
        float x = (lane < num_waves) ? wf_sums[lane] : 0.0f;
        float block_sum = wf_reduce_sum(x);
        if (lane == 0) out[blockIdx.x] = block_sum;
    }
}

// =============== Hostï¼šå¤šè½® reduction ç›´åˆ°å‰©ä¸€ä¸ªæ•° ===============
float run_reduce(const float* d_in, int n,
                 bool use_shuffle,
                 int threads, int iterations) {
    hipEvent_t start, stop;
    HIP_CHECK(hipEventCreate(&start));
    HIP_CHECK(hipEventCreate(&stop));

    int max_blocks = (n + (threads * 2 - 1)) / (threads * 2);
    float* d_buf1 = nullptr;
    float* d_buf2 = nullptr;
    HIP_CHECK(hipMalloc(&d_buf1, max_blocks * sizeof(float)));
    HIP_CHECK(hipMalloc(&d_buf2, max_blocks * sizeof(float)));

    auto launch_once = [&](int cur_n, const float* cur_in, float* cur_out) {
        int blocks = (cur_n + (threads * 2 - 1)) / (threads * 2);

        size_t smem = 0;
        if (use_shuffle) {
            int num_waves = (threads + WARP_SIZE - 1) / WARP_SIZE;
            smem = num_waves * sizeof(float);
            hipLaunchKernelGGL(reduce_shuffle, dim3(blocks), dim3(threads),
                               smem, 0, cur_in, cur_out, cur_n);
        } else {
            smem = threads * sizeof(float);
            hipLaunchKernelGGL(reduce_branchy, dim3(blocks), dim3(threads),
                               smem, 0, cur_in, cur_out, cur_n);
        }
        return blocks;
    };

    // warmup
    {
        int cur_n = n;
        const float* cur_in = d_in;
        float* cur_out = d_buf1;
        while (cur_n > 1) {
            int next_n = launch_once(cur_n, cur_in, cur_out);
            cur_n = next_n;
            cur_in = cur_out;
            cur_out = (cur_out == d_buf1) ? d_buf2 : d_buf1;
        }
        HIP_CHECK(hipDeviceSynchronize());
    }

    HIP_CHECK(hipEventRecord(start));
    for (int it = 0; it < iterations; ++it) {
        int cur_n = n;
        const float* cur_in = d_in;
        float* cur_out = d_buf1;

        while (cur_n > 1) {
            int next_n = launch_once(cur_n, cur_in, cur_out);
            cur_n = next_n;
            cur_in = cur_out;
            cur_out = (cur_out == d_buf1) ? d_buf2 : d_buf1;
        }
    }
    HIP_CHECK(hipEventRecord(stop));
    HIP_CHECK(hipEventSynchronize(stop));

    float ms = 0.0f;
    HIP_CHECK(hipEventElapsedTime(&ms, start, stop));

    HIP_CHECK(hipFree(d_buf1));
    HIP_CHECK(hipFree(d_buf2));
    HIP_CHECK(hipEventDestroy(start));
    HIP_CHECK(hipEventDestroy(stop));
    return ms;
}

int main() {
    int n = 1024 * 1024 * 1000; // 10M
    size_t bytes = n * sizeof(float);

    std::cout << "=== å‘é‡æ±‚å’Œï¼ˆreductionï¼‰å‘æ•£å¯¹æ¯” ===\n";
    std::cout << "æ•°æ®é‡: " << n << " (" << bytes / 1024.0 / 1024.0 << " MB)\n";
    std::cout << "WARP_SIZE(AMD wavefront): " << WARP_SIZE << "\n";

    std::vector<float> h(n, 1.0f);
    float* d_in = nullptr;
    HIP_CHECK(hipMalloc(&d_in, bytes));
    HIP_CHECK(hipMemcpy(d_in, h.data(), bytes, hipMemcpyHostToDevice));

    int threads = 256;
    int iterations = 50;

    std::cout << "\n=== reduce_branchyï¼ˆåˆ†æ”¯+åŒæ­¥ï¼‰===\n";
    float t1 = run_reduce(d_in, n, false, threads, iterations);
    std::cout << "æ€»æ—¶é—´: " << t1 << " ms\n";
    std::cout << "å¹³å‡æ¯æ¬¡: " << t1 / iterations << " ms\n";

    std::cout << "\n=== reduce_shuffleï¼ˆshuffle æ›´å°‘åˆ†æ”¯ï¼‰===\n";
    float t2 = run_reduce(d_in, n, true, threads, iterations);
    std::cout << "æ€»æ—¶é—´: " << t2 << " ms\n";
    std::cout << "å¹³å‡æ¯æ¬¡: " << t2 / iterations << " ms\n";

    std::cout << "\n=== å¯¹æ¯” ===\n";
    float speedup = t1 / t2;
    std::cout << "shuffle ç‰ˆæœ¬åŠ é€Ÿæ¯”: " << speedup << "x\n";
    std::cout << "æ€§èƒ½æå‡: " << (speedup - 1.f) * 100.f << "%\n";

    HIP_CHECK(hipFree(d_in));
    return 0;
}
```

**ç¼–è¯‘å¹¶è¿è¡Œ**ï¼š

```bash
hipcc bench_divergence.cpp -o bench_divergence -O3
./bench_divergence
```

**é¢„æœŸè¾“å‡º**ï¼ˆåœ¨ Radeon 8060S ä¸Šï¼‰ï¼š

```
root@a8c24694ab96:~# ./bench_divergence
=== å‘é‡æ±‚å’Œï¼ˆreductionï¼‰å‘æ•£å¯¹æ¯” ===
æ•°æ®é‡: 1048576000 (4000 MB)
WARP_SIZE(AMD wavefront): 32

=== reduce_branchyï¼ˆåˆ†æ”¯+åŒæ­¥ï¼‰===
æ€»æ—¶é—´: 1104.58 ms
å¹³å‡æ¯æ¬¡: 22.0915 ms

=== reduce_shuffleï¼ˆshuffle æ›´å°‘åˆ†æ”¯ï¼‰===
æ€»æ—¶é—´: 960.202 ms
å¹³å‡æ¯æ¬¡: 19.204 ms

=== å¯¹æ¯” ===
shuffle ç‰ˆæœ¬åŠ é€Ÿæ¯”: 1.15036x
æ€§èƒ½æå‡: 15.0359%
```

<div style="background: #ffebee; border: 1px solid #f44336; border-radius: 8px; padding: 16px; margin: 16px 0;">
  <div style="display: flex; align-items: start;">
    <span style="font-size: 20px; margin-right: 10px;">âš ï¸</span>
    <div>
      <strong style="color: #c62828;">ç»“è®º</strong><br>
      <span style="color: #c62828; line-height: 1.6;">
        ä»æµ‹è¯•ç»“æœå¯ä»¥çœ‹åˆ°ï¼Œä½¿ç”¨ shuffle æŒ‡ä»¤å‡å°‘åˆ†æ”¯å‘æ•£å¸¦æ¥äº† <strong>çº¦15%</strong> çš„æ€§èƒ½æå‡ã€‚<br/>
        è™½ç„¶è¿™ä¸ªä¾‹å­ä¸­æå‡ä¸ç®—å·¨å¤§ï¼Œä½†åœ¨æŸäº›åœºæ™¯ä¸‹åˆ†æ”¯å‘æ•£çš„å½±å“ä¼šæ›´å¤§ã€‚åœ¨å®é™…ç¼–å†™ GPU ä»£ç æ—¶ï¼Œåº”å°½é‡é¿å…åœ¨ wavefront å†…éƒ¨ä½¿ç”¨ if-else åˆ†æ”¯ã€‚
      </span>
    </div>
  </div>
</div>

#### ğŸ’¡ ä¸ºä»€ä¹ˆ 32 æˆ– 64 ä¸ªçº¿ç¨‹ä¸€ç»„ï¼Ÿ

| é€‰é¡¹ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|:---|:---|:---|
| **å¤ªå°‘ï¼ˆå¦‚ 8 ä¸ªï¼‰** | çµæ´» | âŒ ç¡¬ä»¶è°ƒåº¦å¼€é”€å¤§ |
| **å¤ªå¤šï¼ˆå¦‚ 1024 ä¸ªï¼‰** | ååé«˜ | âŒ åˆ†æ”¯å‘æ•£å½±å“å¤ªå¤§ |
| **32/64 ä¸ª** | âœ… åˆšå¥½å¹³è¡¡å¹¶è¡Œåº¦å’Œçµæ´»æ€§ | |

> ğŸ“Œ **æ¶æ„å·®å¼‚**ï¼šè€æ¶æ„ç”¨ 64ï¼Œæ–°æ¶æ„ï¼ˆå¦‚ gfx1151ï¼‰ç”¨ 32

---

### 2.2.3 æ•°æ®å¹¶è¡Œæ€ç»´ï¼šå¦‚ä½•æŠŠé—®é¢˜æ‹†è§£æˆ GPU èƒ½åƒçš„å½¢å¼

#### âœ… å¹¶è¡Œåº¦åˆ†æï¼šä»€ä¹ˆä»»åŠ¡é€‚åˆ GPUï¼Ÿ

| ç‰¹å¾ | é€‚åˆ GPU | ä¸é€‚åˆ GPU |
|:---|:---|:---|
| **æ•°æ®é‡** | å¤§æ•°æ®é›†ï¼ˆ>1000 å…ƒç´ ï¼‰ | å°æ•°æ®ï¼ˆ<100 å…ƒç´ ï¼‰ |
| **æ•°æ®ä¾èµ–** | æ•°æ®ä¹‹é—´ç‹¬ç«‹ | æ•°æ®ä¹‹é—´æœ‰å¤æ‚ä¾èµ– |
| **è®¡ç®—æ¨¡å¼** | è§„åˆ™é‡å¤ï¼ˆçŸ©é˜µã€å·ç§¯ï¼‰ | å¤æ‚é€»è¾‘ï¼ˆé€’å½’ã€å›æº¯ï¼‰ |
| **åˆ†æ”¯** | å¾ˆå°‘ if-else | å¾ˆå¤š if-else |
| **å†…å­˜è®¿é—®** | è¿ç»­è®¿é—® | éšæœºè·³è½¬ |

#### ğŸ“ ç¤ºä¾‹å¯¹æ¯”

**âœ… é€‚åˆ GPU**ï¼š
- çŸ©é˜µä¹˜æ³•ï¼šæ¯ä¸ªå…ƒç´ ç‹¬ç«‹è®¡ç®—
- å›¾åƒå·ç§¯ï¼šæ¯ä¸ªåƒç´ ç‹¬ç«‹å¤„ç†
- å‘é‡åŠ æ³•ï¼š`c[i] = a[i] + b[i]`

**âŒ ä¸é€‚åˆ GPU**ï¼š
- æ–æ³¢é‚£å¥‘æ•°åˆ—ï¼š`f(n) = f(n-1) + f(n-2)` ï¼ˆæœ‰ä¾èµ–ï¼‰
- å¿«é€Ÿæ’åºï¼šåˆ†æ”¯å¤ªå¤š
- å›¾éå†ï¼šå†…å­˜è®¿é—®ä¸è§„åˆ™

#### ğŸš€ å†…å­˜è®¿é—®æ¨¡å¼ï¼šåˆå¹¶è®¿é—® vs éšæœºè®¿é—®

**åˆå¹¶è®¿é—®ï¼ˆCoalesced Accessï¼‰- å¿«**ï¼š

```cpp
__global__ void good_access(float* data) {
    int i = threadIdx.x;  // 0, 1, 2, 3, ...
    // è¿ç»­çš„çº¿ç¨‹è®¿é—®è¿ç»­çš„å†…å­˜ â†’ ä¸€æ¬¡å†…å­˜äº‹åŠ¡
    float x = data[i];
}
```

**å†…å­˜è®¿é—®æ¨¡å¼**ï¼š
```
çº¿ç¨‹0 â†’ data[0]  (å­—èŠ‚ 0-3)
çº¿ç¨‹1 â†’ data[1]  (å­—èŠ‚ 4-7)
çº¿ç¨‹2 â†’ data[2]  (å­—èŠ‚ 8-11)
çº¿ç¨‹3 â†’ data[3]  (å­—èŠ‚ 12-15)
...
çº¿ç¨‹31 â†’ data[31] (å­—èŠ‚ 124-127)
æ€»å…±éœ€è¦ä¸€æ¬¡å†…å­˜äº‹åŠ¡ï¼Œä¸€æ¬¡è¯» 128 å­—èŠ‚ï¼ŒæœåŠ¡ 32 ä¸ªçº¿ç¨‹ âœ…
```

**éšæœºè®¿é—®ï¼ˆStrided Accessï¼‰- æ…¢**ï¼š

```cpp
__global__ void bad_access(float* data) {
    int i = threadIdx.x;
    // çº¿ç¨‹0è®¿é—®data[0]ï¼Œçº¿ç¨‹1è®¿é—®data[128]...
    float x = data[i * 128];  // è·¨åº¦128
}
```

**å†…å­˜è®¿é—®æ¨¡å¼**ï¼š
```
çº¿ç¨‹0 â†’ data[0]    â†’ éœ€è¦ 1 æ¬¡ 128 å­—èŠ‚äº‹åŠ¡
çº¿ç¨‹1 â†’ data[128]  â†’ éœ€è¦ 1 æ¬¡ 128 å­—èŠ‚äº‹åŠ¡
çº¿ç¨‹2 â†’ data[256]  â†’ éœ€è¦ 1 æ¬¡ 128 å­—èŠ‚äº‹åŠ¡
...
çº¿ç¨‹31 â†’ data[3968] â†’ éœ€è¦ 1 æ¬¡ 128 å­—èŠ‚äº‹åŠ¡
æ€»å…±ï¼š32 æ¬¡å†…å­˜äº‹åŠ¡ï¼
```

---

## 2.3 èµ°è¿› AMD GPU å†…éƒ¨ï¼šç¡¬ä»¶æ¶æ„è¯¦è§£

ç°åœ¨æˆ‘ä»¬æ·±å…¥ AMD GPU çš„ç¡¬ä»¶ï¼Œäº†è§£å®ƒçš„"å†…è„"æ˜¯å¦‚ä½•ç»„ç»‡çš„ã€‚

### 2.3.1 Compute Unit (CU)ï¼šGPU çš„"å¹²æ´»å°ç»„"

#### ğŸ”§ CU å†…éƒ¨ç»“æ„

ä¸€ä¸ª Compute Unit (CU) æ˜¯ GPU çš„åŸºæœ¬è®¡ç®—å•å…ƒï¼Œå®ƒåŒ…å«ä»¥ä¸‹ç»„ä»¶ï¼š

```mermaid
graph TB
    subgraph CU["ğŸ­ Compute Unit (CU)"]
        direction TB

        SIMD["âš¡ SIMD Units<br/>_________________<br/>â€¢ æ•°é‡: 4 ä¸ª<br/>â€¢ æ¯ä¸ª SIMD: 64 ä¸ª ALU<br/>â€¢ æ€»è®¡: 256 ä¸ªæ ‡é‡ ALU<br/>â€¢ æ€§èƒ½: æ¯å‘¨æœŸ 256 ä¸ª float"]

        REG["ğŸ’¾ Register File<br/>_________________<br/>â€¢ ç±»å‹: VGPR<br/>â€¢ æ•°é‡: æ•°åƒä¸ª<br/>â€¢ å¤§å°: æ¯ä¸ªå¯„å­˜å™¨ 32 å­—èŠ‚<br/>â€¢ ç”¨é€”: å­˜å‚¨çº¿ç¨‹ç§æœ‰æ•°æ®"]

        LDS["ğŸ”² LDS Local Data Store<br/>_________________<br/>â€¢ å®¹é‡: 128 KB<br/>â€¢ å»¶è¿Ÿ: 20-40 cycles<br/>â€¢ é€Ÿåº¦: æ¯”æ˜¾å­˜å¿« 10 å€<br/>â€¢ ç”¨é€”: å…±äº«å†…å­˜"]

        CTRL["ğŸ›ï¸ æ§åˆ¶å•å…ƒ<br/>_________________<br/>â€¢ Instruction Cache: æŒ‡ä»¤ç¼“å­˜<br/>â€¢ Scalar Unit: æ ‡é‡å¤„ç†<br/>â€¢ ç”¨é€”: åœ°å€è®¡ç®—ã€åˆ†æ”¯åˆ¤æ–­<br/>â€¢ ä¼˜åŠ¿: å‡å°‘å‘é‡å•å…ƒè´Ÿæ‹…"]
    end

    style SIMD fill:#fff3e0,stroke:#ff9800,stroke-width:3px
    style REG fill:#e8f5e9,stroke:#4caf50,stroke-width:3px
    style LDS fill:#f3e5f5,stroke:#9c27b0,stroke-width:3px
    style CTRL fill:#e3f2fd,stroke:#2196f3,stroke-width:3px
    style CU fill:#f5f7fa,stroke:#1976d2,stroke-width:3px
```

<div style="background: #e3f2fd; border: 1px solid #2196f3; border-radius: 8px; padding: 12px; margin: 16px 0;">
  <div style="display: flex; align-items: start;">
    <span style="font-size: 18px; margin-right: 10px;">ğŸ’¡</span>
    <div>
      <strong style="color: #1565c0;">ä»¥ Radeon 8060S (gfx1151 æ¶æ„) ä¸ºä¾‹</strong>
    </div>
  </div>
</div>
**ğŸ“‹ ç»„ä»¶è¯¦æƒ…è¡¨**ï¼š

| ç»„ä»¶           | å…¨ç§°             | åŠŸèƒ½æ¯”å–»                                                     | å…³é”®ç‰¹æ€§                                       |
| :------------- | :--------------- | :----------------------------------------------------------- | :--------------------------------------------- |
| **SIMD Units** | å•æŒ‡ä»¤å¤šæ•°æ®å•å…ƒ | **æ¬ç –å·¥**ã€‚å¤§å®¶å¬åŒä¸€ä¸ªå£ä»¤ï¼Œæ¬ä¸åŒçš„ç –ã€‚                   | æ¯ä¸ª CU é€šå¸¸æœ‰ 4 ä¸ª SIMDï¼Œè´Ÿè´£æ‰§è¡Œå‘é‡æŒ‡ä»¤ã€‚   |
| **VGPR**       | å‘é‡é€šç”¨å¯„å­˜å™¨   | **å·¥äººçš„èƒŒåŒ…**ã€‚æ¯ä¸ªçº¿ç¨‹ç§æœ‰çš„å­˜å‚¨ç©ºé—´ã€‚                     | **æå¿«ä½†æè´µ**ã€‚æ•°é‡é™åˆ¶äº†èƒ½åŒæ—¶å¼€å·¥çš„å·¥äººæ•°ã€‚ |
| **SGPR**       | æ ‡é‡é€šç”¨å¯„å­˜å™¨   | **é˜Ÿé•¿çš„è®°äº‹æœ¬**ã€‚å­˜å‚¨æ‰€æœ‰çº¿ç¨‹å…±ç”¨çš„æ•°æ®ï¼ˆå¦‚å¾ªç¯è®¡æ•°ã€å‡½æ•°å‚æ•°ï¼‰ã€‚ | è¿™é‡Œçš„æ“ä½œç”±æ ‡é‡å•å…ƒå¤„ç†ï¼Œä¸å ç”¨å‘é‡å•å…ƒèµ„æºã€‚ |
| **LDS**        | Local Data Store | **é˜Ÿå†…çš„å°ä»“åº“**ã€‚åŒä¸€ä¸ª CU å†…çš„çº¿ç¨‹å¯ä»¥é€šè¿‡å®ƒäº¤æ¢æ•°æ®ã€‚     | é€Ÿåº¦æ¯”æ˜¾å­˜å¿« 10-20 å€ï¼Œç”¨äº Block å†…é€šè®¯ã€‚     |

> ğŸ’¡ **æ¶æ„å·®å¼‚å°è´´å£«**ï¼š
>
> - **CDNA (å¦‚ MI300)**ï¼šä¸“ä¸ºè®¡ç®—è®¾è®¡ï¼Œé€šå¸¸é‡‡ç”¨ **Wave64** æ¨¡å¼ï¼ˆ64ä¸ªçº¿ç¨‹ä¸€ç»„ï¼‰ã€‚
> - **RDNA (å¦‚ Radeon 7900)**ï¼šä¸“ä¸ºæ¸¸æˆè®¾è®¡ï¼Œé€šå¸¸é‡‡ç”¨ **Wave32** æ¨¡å¼ï¼ˆ32ä¸ªçº¿ç¨‹ä¸€ç»„ï¼‰ï¼Œä½†åœ¨è®¡ç®—ä»»åŠ¡ä¸­ä¹Ÿå¯ä»¥ç¼–è¯‘ä¸º Wave64ã€‚

#### ğŸ“Š å…³é”®å‚æ•°

| å‚æ•° | è¯´æ˜ |
|:---|:---|
| **CU æ•°é‡** | Radeon 8060S æœ‰æ•°åä¸ª CUï¼ˆå…·ä½“æ•°é‡å–å†³äºèŠ¯ç‰‡å‹å·ï¼‰ |
| **æ¯ä¸ª CU çš„ SIMD æ•°é‡** | 2-4 ä¸ª | æ¯ä¸ª SIMD æœ‰ 32/64 ä¸ª ALU |
| **æ¯ä¸ª CU çš„ ALU æ€»æ•°** | æ•°ç™¾ä¸ª | å–å†³äºæ¶æ„ |
| **ç†è®ºå³°å€¼ FP32 åå** | æ•° TFLOPS çº§åˆ« | å–å†³äº CU æ•°é‡å’Œé¢‘ç‡ |

#### ğŸ”¢ ä¸€ä¸ª CU èƒ½åŒæ—¶è·‘å¤šå°‘çº¿ç¨‹ï¼Ÿ

è¿™æ˜¯ GPU æ€§èƒ½ä¼˜åŒ–çš„å…³é”®é—®é¢˜ã€‚æˆ‘ä»¬éœ€è¦ç†è§£å‡ ä¸ªæ¦‚å¿µï¼š

| æ¦‚å¿µ | è¯´æ˜ |
|:---|:---|
| **Work-Itemï¼ˆçº¿ç¨‹ï¼‰** | æœ€å°çš„æ‰§è¡Œå•ä½ |
| **Work-Groupï¼ˆçº¿ç¨‹å—ï¼‰** | ä¸€ç»„çº¿ç¨‹ï¼Œå¯ä»¥å…±äº« LDS |
| **Wavefrontï¼ˆæ³¢å‰ï¼‰** | 32/64 ä¸ªçº¿ç¨‹ï¼ˆå–å†³äºæ¶æ„ï¼‰ï¼Œä¸€èµ·æ‰§è¡Œç›¸åŒæŒ‡ä»¤ |

**å¹¶å‘è®¡ç®—**ï¼š

- æ¯ä¸ª wavefront éœ€è¦ï¼š
  - 32 æˆ– 64 ä¸ª VGPRï¼ˆå‡è®¾æ¯ä¸ªçº¿ç¨‹ç”¨ 1 ä¸ªå¯„å­˜å™¨ï¼‰
  - ä¸€å®šæ•°é‡çš„ LDSï¼ˆå¦‚æœä½¿ç”¨ï¼‰
  - æŒ‡ä»¤æ§½ä½

- æ¯ä¸ª CU å¯ä»¥åŒæ—¶è·‘ **å¤šä¸ª wavefront**ï¼ˆå…·ä½“æ•°é‡å–å†³äºæ¶æ„ï¼‰

**æ€»å¹¶å‘çº¿ç¨‹æ•°**ï¼š
```
CU æ•°é‡ Ã— wavefront/CU Ã— 32/64 thread/wavefront = æ•°ä¸‡åˆ°æ•°åä¸‡ä¸ªçº¿ç¨‹
```

#### ğŸ¤” ä¸ºä»€ä¹ˆçº¿ç¨‹æŸè¦è¶…è¿‡ç‰©ç†æ ¸å¿ƒæ•°ï¼Ÿ

- GPU çš„ç‰©ç† ALU æ•°é‡æœ‰é™
- ä½†èƒ½è·‘è¿œè¶…ç‰©ç†æ ¸å¿ƒæ•°çš„çº¿ç¨‹ï¼ˆ**æ•°å€åˆ°åå€**ï¼ï¼‰
- **åŸå› **ï¼šå†…å­˜å»¶è¿Ÿéšè—
  - å½“ Wave A ç­‰å¾…å†…å­˜ï¼ˆéœ€è¦ 500 ä¸ªå‘¨æœŸï¼‰æ—¶ï¼Œè°ƒåº¦å™¨ç¬é—´åˆ‡æ¢åˆ° Wave B æ‰§è¡Œè®¡ç®—ã€‚
  - **åªè¦æ’é˜Ÿçš„ Wave è¶³å¤Ÿå¤šï¼ŒCU å°±æ°¸è¿œä¸ä¼šåœä¸‹æ¥ã€‚**

#### ğŸ“ˆ Occupancyï¼šèµ„æºåˆ©ç”¨ç‡

**å…¬å¼**ï¼šOccupancy = å®é™…å¹¶å‘ wavefront æ•° / ç†è®ºæœ€å¤§ wavefront æ•°

**å½±å“å› ç´ **ï¼š

| èµ„æº | é™åˆ¶å› ç´  | ä¼˜åŒ–æ–¹æ³• |
|:---|:---|:---|
| **VGPR** | æ¯ä¸ªçº¿ç¨‹ç”¨å¤ªå¤šå¯„å­˜å™¨ | å‡å°‘å¯„å­˜å™¨ä½¿ç”¨ |
| **LDS** | Work-group ç”¨å¤ªå¤š LDS | å‡å° LDS ä½¿ç”¨ |
| **æŒ‡ä»¤æ§½ä½** | Code å¤ªå¤§ | å‡å°ä»£ç å¤§å° |

**ç¤ºä¾‹å¯¹æ¯”**ï¼š

```cpp
// å¯„å­˜å™¨ä½¿ç”¨å¤š â†’ Occupancy ä½
__global__ void heavy_register(float* data) {
    float a, b, c, d, e, f, g, h;  // 8ä¸ªå¯„å­˜å™¨
    // ... å¤æ‚è®¡ç®—
}

// å¯„å­˜å™¨ä½¿ç”¨å°‘ â†’ Occupancy é«˜
__global__ void light_register(float* data) {
    float a = data[threadIdx.x];
    a *= 2.0f;
    data[threadIdx.x] = a;
}
```

> ğŸ¯ **å®æˆ˜åˆ†æ**ï¼šä½¿ç”¨ rocprof å·¥å…·åˆ†æ GPU ç¨‹åºçš„ Occupancy å’Œæ€§èƒ½æŒ‡æ ‡

**å®‰è£… rocprof**ï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰ï¼š

```bash
# Ubuntu/Debian
sudo apt install hip-dev rocprofiler-dev roctracer-dev -y

# éªŒè¯å®‰è£…
/opt/rocm/bin/rocprof --version
```

**åˆ›å»ºæµ‹è¯•ç¨‹åº** `test_occupancy.cpp`ï¼š

```cpp
// file: test_occupancy_v2.cpp
#include <hip/hip_runtime.h>
#include <iostream>
#include <cstdlib>
#include <cmath>

#define HIP_CHECK(call) \
    do { \
        hipError_t err = call; \
        if (err != hipSuccess) { \
            std::cerr << "HIP Error: " << hipGetErrorString(err) \
                      << " at line " << __LINE__ << std::endl; \
            std::exit(EXIT_FAILURE); \
        } \
    } while (0)


// ä½¿ç”¨ __launch_bounds__ å¯ä»¥è¾…åŠ©æ§åˆ¶ï¼Œä½†è¿™é‡Œæˆ‘ä»¬ä¸»è¦é ä»£ç é€»è¾‘
// ç›®æ ‡ï¼šè®©æ¯ä¸ªçº¿ç¨‹å ç”¨ >64 ä¸ª VGPRï¼Œä»è€Œå‡å°‘ Wavefront å¹¶è¡Œåº¦
__global__ void heavy_kernel(float* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        // ä½¿ç”¨ volatile ç¦æ­¢ç¼–è¯‘å™¨ä¼˜åŒ–æ‰ä¸­é—´å˜é‡
        // å¹¶ä¸”å®šä¹‰å¤§é‡å˜é‡ï¼Œæ¨¡æ‹Ÿå¤æ‚çš„ä¸­é—´çŠ¶æ€
        volatile float v0 = data[idx];
        volatile float v1 = v0 * 1.01f;
        volatile float v2 = v1 * 1.02f;
        volatile float v3 = v2 * 1.03f;
        volatile float v4 = v3 * 1.04f;
        volatile float v5 = v4 * 1.05f;
        volatile float v6 = v5 * 1.06f;
        volatile float v7 = v6 * 1.07f;
        volatile float v8 = v7 * 1.08f;
        volatile float v9 = v8 * 1.09f;
        
        // å¢åŠ æ›´å¤šå˜é‡ä»¥é€šè¿‡å¯„å­˜å™¨æº¢å‡ºé˜ˆå€¼
        volatile float a[20];
        #pragma unroll
        for(int i=0; i<20; i++) {
            a[i] = v0 * (float)(i+1);
        }

        // å¤æ‚çš„æ•°å­¦è¿ç®—ï¼Œå¼ºåˆ¶ä¾èµ–æ‰€æœ‰å˜é‡
        // ä½¿å¾—ç¼–è¯‘å™¨å¿…é¡»ä¿ç•™å®ƒä»¬çš„å€¼ç›´åˆ°æœ€å
        float sum = 0.0f;
        #pragma unroll
        for(int i=0; i<20; i++) {
            sum += a[i] * v1;
            sum -= v2 * v3;
            sum += v4 / (v5 + 1e-6f);
            sum *= (v6 + v7);
        }
        
        // é˜²æ­¢è¢«ä¼˜åŒ–ï¼šå¿…é¡»å†™å›ç»“æœ
        data[idx] = sum + v8 + v9;
    }
}

// ==========================================
// å¯¹ç…§ç»„ï¼šä½å¯„å­˜å™¨å ç”¨ Kernel
// ==========================================
__global__ void light_kernel(float* data, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        float val = data[idx];
        val *= 2.0f;
        data[idx] = val;
    }
}

int main() {
    // å¢åŠ æ•°æ®é‡ä»¥è·å¾—æ›´ç¨³å®šçš„æ€§èƒ½æ•°æ®
    int n = 1024 * 1024 * 20; // 20M floats
    size_t bytes = n * sizeof(float);

    float *d_data;
    HIP_CHECK(hipMalloc(&d_data, bytes));

    // ä½¿ç”¨ 256 çº¿ç¨‹/Blockï¼Œè¿™æ˜¯ AMD GPU çš„å…¸å‹é…ç½®
    const int threads = 256;
    const int blocks = (n + threads - 1) / threads;

    std::cout << "Data size: " << n << " elements (" << bytes/1024/1024 << " MB)" << std::endl;

    // é¢„çƒ­ GPU
    hipLaunchKernelGGL(light_kernel, dim3(blocks), dim3(threads), 0, 0, d_data, n);
    HIP_CHECK(hipDeviceSynchronize());

    std::cout << "=== Running heavy_kernel (High Register Pressure) ===" << std::endl;
    // è¿è¡Œå¤šæ¬¡ä»¥è·å–å¹³å‡å€¼
    for(int i=0; i<5; i++) {
        hipLaunchKernelGGL(heavy_kernel, dim3(blocks), dim3(threads), 0, 0, d_data, n);
    }
    HIP_CHECK(hipDeviceSynchronize());

    std::cout << "\n=== Running light_kernel (Low Register Pressure) ===" << std::endl;
    for(int i=0; i<5; i++) {
        hipLaunchKernelGGL(light_kernel, dim3(blocks), dim3(threads), 0, 0, d_data, n);
    }
    HIP_CHECK(hipDeviceSynchronize());

    HIP_CHECK(hipFree(d_data));
    return 0;
}
```

**ç¼–è¯‘å¹¶ä½¿ç”¨ rocprof åˆ†æ**ï¼š

```bash
# ç¼–è¯‘
hipcc test_occupancy.cpp -o test_occupancy -O3

# è¿è¡Œåˆ†æï¼ˆä¼šç”Ÿæˆ results.csvï¼‰
rocprof --stats ./test_occupancy

#æŸ¥çœ‹ç”Ÿæˆçš„ CSV æ–‡ä»¶
cat results.csv
```

>root@aimax395:/lwh/2.3# cat results.csv
>"Index","KernelName","gpu-id","queue-id","queue-index","pid","tid","grd","wgr","lds","scr","arch_vgpr","accum_vgpr","sgpr","wave_size","sig","obj","DispatchNs","BeginNs","EndNs","CompleteNs","DurationNs"
>0,"light_kernel(float*, int) [clone .kd]",1,0,0,937,937,20971520,256,0,0,8,0,128,32,"0x0","0x7073a07c8d40",363394501327600,363394501507848,363394502312007,363394502319511,804159
>1,"heavy_kernel(float*, int) [clone .kd]",1,0,2,937,937,20971520,256,0,144,72,0,128,32,"0x0","0x7073a07c8d00",363394502348015,363394502772909,363394537269967,363394537278002,34497058
>2,"heavy_kernel(float*, int) [clone .kd]",1,0,3,937,937,20971520,256,0,144,72,0,128,32,"0x0","0x7073a07c8d00",363394502374495,363394537272532,363394571779648,363394571779471,34507116
>3,"heavy_kernel(float*, int) [clone .kd]",1,0,4,937,937,20971520,256,0,144,72,0,128,32,"0x0","0x7073a07c8d00",363394502377701,363394571782075,363394606233010,363394606239912,34450935
>4,"heavy_kernel(float*, int) [clone .kd]",1,0,5,937,937,20971520,256,0,144,72,0,128,32,"0x0","0x7073a07c8d00",363394502381037,363394606235454,363394640752233,363394640752522,34516779
>5,"heavy_kernel(float*, int) [clone .kd]",1,0,6,937,937,20971520,256,0,144,72,0,128,32,"0x0","0x7073a07c8d00",363394502384404,363394640754902,363394675368628,363394675376041,34613726
>6,"light_kernel(float*, int) [clone .kd]",1,0,8,937,937,20971520,256,0,0,8,0,128,32,"0x0","0x7073a07c8d40",363394675377203,363394675384847,363394676106849,363394676107360,722002
>7,"light_kernel(float*, int) [clone .kd]",1,0,9,937,937,20971520,256,0,0,8,0,128,32,"0x0","0x7073a07c8d40",363394675381341,363394676109454,363394676832498,363394676832298,723044
>8,"light_kernel(float*, int) [clone .kd]",1,0,10,937,937,20971520,256,0,0,8,0,128,32,"0x0","0x7073a07c8d40",363394675383876,363394676834942,363394677548649,363394677548559,713707
>9,"light_kernel(float*, int) [clone .kd]",1,0,11,937,937,20971520,256,0,0,8,0,128,32,"0x0","0x7073a07c8d40",363394675386390,363394677551053,363394678268366,363394678268527,717313
>10,"light_kernel(float*, int) [clone .kd]",1,0,12,937,937,20971520,256,0,0,8,0,128,32,"0x0","0x7073a07c8d40",363394675389386,363394678270851,363394678991129,363394678991491,720278

**å…³é”®æŒ‡æ ‡è§£è¯»**ï¼š

|       æŒ‡æ ‡ (Metric)        | Light Kernel (è½»é‡çº§) |  Heavy Kernel (é‡é‡çº§)  |            å·®å¼‚å€æ•°            |
| :------------------------: | :-------------------: | :---------------------: | :----------------------------: |
| **arch_vgpr (å‘é‡å¯„å­˜å™¨)** |         **8**         |         **72**          | **9å€å·®è·ï¼** (è¿™å°±æ˜¯æ ¸å¿ƒåŸå› ) |
| **DurationNs (æ‰§è¡Œæ—¶é—´)**  | ~720,000 ns (0.72ms)  | ~34,500,000 ns (34.5ms) |        **~48å€å·®è·ï¼**         |
|  **scr (Scratch Memory)**  |           0           |         **144**         |     å‘ç”Ÿå¯„å­˜å™¨æº¢å‡º (Spill)     |

**è®¡ç®—ç†è®º Occupancy**ï¼š

å¯¹äº gfx1151 æ¶æ„ï¼ˆRadeon 8060Sï¼‰ï¼š
- æ¯ä¸ª CU çš„ VGPR æ€»æ•°ï¼š1024
- æ¯ä¸ª CU çš„ LDS æ€»æ•°ï¼š64 KB
- æ¯ä¸ª CU æœ€å¤§ wavefront æ•°ï¼š32ï¼ˆç†è®ºå€¼ï¼‰

```python
# file: calc_occupancy.py

def calculate_occupancy_arch1151(vgpr_per_thread, lds_per_workgroup, threads_per_wg):
    # ==============================
    # 1. ç¡¬ä»¶å‚æ•° (RDNA3 å…¸å‹å€¼)
    # ==============================
    # æ ¸å¿ƒå‚æ•°ï¼šæ¯ä¸ª SIMD çš„ VGPR æ€»æ•° (ç‰©ç†)
    # 64KB / 4bytes = 16384 ä¸ªå¯„å­˜å™¨
    # ä½†ç”±äºæ˜¯ Wave32ï¼Œæ¯ä¸ª Wave å ç”¨ 32 ä¸ªé€šé“
    # é€»è¾‘ä¸Šç›¸å½“äºæ¯ä¸ªé€šé“æœ‰ 16384 / 32 = 512 ä¸ª VGPR æ·±åº¦ï¼Ÿä¸å®Œå…¨æ˜¯ã€‚
    # 
    # ç®€åŒ–æ¨¡å‹ (Standard RDNA3 Model):
    # ä¸€ä¸ª SIMD å•å…ƒæœ€å¤šå®¹çº³ 32 ä¸ª Waveã€‚
    # VGPR æ€»æ± å­å¤§å°é€šå¸¸è¢«æ ‡å‡†åŒ–ä¸ºï¼šæ¯ Wave æœ€å¤§ 256 ä¸ª VGPR * 32 Wave = 8192 ä¸ªé€»è¾‘å—ï¼Ÿ
    # 
    # æˆ‘ä»¬ä½¿ç”¨å®˜æ–¹æ¨èçš„è®¡ç®—å…¬å¼ï¼š
    # Total VGPR Budget per SIMD = 1536 (è¿™æ˜¯å½’ä¸€åŒ–åˆ° Wave64 çš„æ•°å€¼ï¼ŒRDNA3 Wave32 åˆ™æ˜¯ 3072)
    # ä½†ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬ç›´æ¥ç”¨ç‰©ç†å¯„å­˜å™¨æ•°è®¡ç®—ï¼š
    total_physical_vgprs = 65536 // 4  # 16384
    
    wave_size = 32  # RDNA3 é»˜è®¤ä¸º Wave32
    max_waves_per_simd = 32 # æ¯ä¸ª SIMD æœ€å¤š 32 ä¸ª Wave
    
    # ==============================
    # 2. èµ„æºåˆ†é…ä¸å¯¹é½ (Granularity)
    # ==============================
    # å¯„å­˜å™¨ä¸æ˜¯ä¸€ä¸ªä¸€ä¸ªåˆ†çš„ï¼Œè€Œæ˜¯æŒ‰å—åˆ†çš„ã€‚
    # RDNA3 Wave32 æ¨¡å¼ä¸‹ï¼Œé€šå¸¸æŒ‰ 8 æˆ– 16 å¯¹é½ã€‚æˆ‘ä»¬æŒ‰ 8 è®¡ç®—ã€‚
    vgpr_granularity = 8
    
    # è®¡ç®—æ¯ä¸ª Wave å®é™…æ¶ˆè€—çš„ VGPR å—
    # ä¾‹å¦‚ï¼šéœ€è¦ 72 ä¸ªï¼Œå®é™…åˆ†é… 72 (å¦‚æœæ˜¯ 8 çš„å€æ•°)
    aligned_vgpr = ((vgpr_per_thread + vgpr_granularity - 1) // vgpr_granularity) * vgpr_granularity
    
    # ä¸€ä¸ª Wave æ¶ˆè€—çš„ç‰©ç†å¯„å­˜å™¨æ€»æ•° = å¯¹é½åçš„ VGPR * çº¿ç¨‹æ•°
    vgpr_per_wave = aligned_vgpr * wave_size

    # ==============================
    # 3. è®¡ç®—é™åˆ¶
    # ==============================
    
    # é™åˆ¶ A: VGPR å¯¼è‡´çš„ Wave æ•°é‡é™åˆ¶
    if vgpr_per_wave == 0: 
        waves_by_vgpr = max_waves_per_simd
    else:
        waves_by_vgpr = total_physical_vgprs // vgpr_per_wave
    
    # é™åˆ¶ B: LDS å¯¼è‡´çš„é™åˆ¶ (æ¯ä¸ª CU 64KB)
    # å‡è®¾ä¸€ä¸ª Workgroup åˆ†é…åˆ°ä¸€ä¸ª SIMD (ç®€åŒ–)
    # å®é™…ä¸Š LDS æ˜¯ CU çº§å…±äº«çš„ï¼Œä½†è¿™é‡Œç®€åŒ–ä¸ºå• SIMD è§†è§’
    max_lds_bytes = 65536
    waves_per_wg = (threads_per_wg + wave_size - 1) // wave_size
    
    if lds_per_workgroup > 0:
        max_wgs_by_lds = max_lds_bytes // lds_per_workgroup
        waves_by_lds = max_wgs_by_lds * waves_per_wg
    else:
        waves_by_lds = max_waves_per_simd

    # é™åˆ¶ C: ç¡¬ä»¶æœ€å¤§ Wave æ•°
    # Workgroup å¤§å°å¯¼è‡´çš„é™åˆ¶ï¼š
    # å¦‚æœä¸€ä¸ª WG æœ‰ 256 çº¿ç¨‹ (8 Waves)ï¼Œé‚£ä¹ˆ SIMD å¿…é¡»èƒ½å®¹çº³æ•´æ•°ä¸ª WG
    # max_waves å¿…é¡»å‘ä¸‹å–æ•´åˆ° WG çš„å€æ•°
    limit_waves = min(waves_by_vgpr, waves_by_lds, max_waves_per_simd)
    
    # å‘ä¸‹å–æ•´åˆ° Workgroup çš„ Wave æ•°
    active_waves = (limit_waves // waves_per_wg) * waves_per_wg

    # ==============================
    # 4. æœ€ç»ˆç»“æœ
    # ==============================
    occupancy_pct = (active_waves / max_waves_per_simd) * 100
    
    return active_waves, occupancy_pct

# ==========================================
# è¿è¡Œè®¡ç®—
# ==========================================

print("=== ç†è®º Occupancy è®¡ç®— (RDNA3 / gfx1151) ===\n")

# Case 1: Heavy Kernel
# æ ¹æ®ä½ çš„ results.csv: VGPR=72, LDS=0, Threads=256
vgpr_heavy = 72
occ_heavy, pct_heavy = calculate_occupancy_arch1151(vgpr_heavy, 0, 256)
print(f"Heavy Kernel (VGPR={vgpr_heavy}):")
print(f"  -> ç†è®º Active Waves: {occ_heavy} / 32")
print(f"  -> ç†è®º Occupancy:    {pct_heavy:.1f}%")
print(f"  -> åˆ†æ: è™½ç„¶ Occupancy è¿˜æœ‰ {pct_heavy:.1f}%ï¼Œä½†æ€§èƒ½ä¾ç„¶å¾ˆå·®ï¼Œ")
print(f"           è¿™æ˜¯å› ä¸ºå‘ç”Ÿäº†ä¸¥é‡çš„ Register Spill (scr=144)ã€‚\n")

# Case 2: Light Kernel
# æ ¹æ® results.csv: VGPR=8, LDS=0, Threads=256
vgpr_light = 8
occ_light, pct_light = calculate_occupancy_arch1151(vgpr_light, 0, 256)
print(f"Light Kernel (VGPR={vgpr_light}):")
print(f"  -> ç†è®º Active Waves: {occ_light} / 32")
print(f"  -> ç†è®º Occupancy:    {pct_light:.1f}%")

```

**è¾“å‡º**ï¼š

```
Heavy Kernel (VGPR=72):
  -> CU å®¹é‡é™åˆ¶: æœ€å¤šå®¹çº³ 14 ä¸ª Waves (VGPRé™åˆ¶)
  -> Workgroupéœ€æ±‚: æ¯ä¸ª WG éœ€è¦ 8 ä¸ª Waves
  -> å®é™…è°ƒåº¦: èƒ½å¡è¿› 1 ä¸ª Workgroup
  -> Active Waves: 8 / 64
  -> ç†è®º Occupancy: 12.5%

------------------------------

Light Kernel (VGPR=8):
  -> Active Waves: 64 / 64
  -> ç†è®º Occupancy: 100.0%
```

**ç»ˆææ€§èƒ½åˆ†ææŠ¥å‘Š**

| æ€§èƒ½æ€æ‰‹                 | ç°è±¡ (Data)          | è§£é‡Š (Explanation)                                           |
| :----------------------- | :------------------- | :----------------------------------------------------------- |
| **1. æä½çš„å¹¶è¡Œåº¦**      | **Occupancy: 12.5%** | ä½ çš„ CU é‡Œæœ‰ **87.5% çš„è®¡ç®—èµ„æºæ˜¯é—²ç½®çš„**ï¼<br>åŸæœ¬å¯ä»¥è·‘ 64 ä¸ª Waveï¼Œç°åœ¨åªèƒ½è·‘ 8 ä¸ªã€‚GPU æ— æ³•é€šè¿‡åˆ‡æ¢çº¿ç¨‹æ¥æ©ç›–å†…å­˜å»¶è¿Ÿï¼Œåªèƒ½å¹²ç­‰ã€‚ |
| **2. èµ„æºç¢ç‰‡åŒ–**        | **Waves: 8 / 14**    | è™½ç„¶ VGPR å¤Ÿæ”¾ 14 ä¸ª Waveï¼Œä½†å› ä¸ºä½ çš„ Workgroup å¼ºåˆ¶è¦æ±‚ 8 ä¸ª Wave ä¸€ç»„ï¼Œå¯¼è‡´å‰©ä¸‹çš„ 6 ä¸ªç©ºä½å¡ä¸ä¸‹ç¬¬äºŒä¸ª Workgroupï¼Œç™½ç™½æµªè´¹äº†ã€‚ |
| **3. å†…å­˜æº¢å‡º (æœ€è‡´å‘½)** | **scr: 144 bytes**   | å³ä½¿åªæœ‰ 12.5% çš„ Occupancyï¼Œå¯„å­˜å™¨è¿˜æ˜¯ä¸å¤Ÿç”¨ï¼ç¼–è¯‘å™¨è¢«è¿«æŠŠéƒ¨åˆ†å˜é‡â€œæº¢å‡ºâ€åˆ°æ…¢é€Ÿçš„ Scratch Memory (æ˜¾å­˜/L1) ä¸­ï¼Œå¯¼è‡´è¯»å†™é€Ÿåº¦ä» **å‡  TB/s é™åˆ° å‡ ç™¾ GB/s**ã€‚ |

<div style="background: #e3f2fd; border: 1px solid #2196f3; border-radius: 8px; padding: 16px; margin: 16px 0;">
  <div style="display: flex; align-items: start;">
    <span style="font-size: 20px; margin-right: 10px;">ğŸ“Š</span>
    <div>
      <strong style="color: #1565c0;">rocprof å¸¸ç”¨å‘½ä»¤</strong><br>
      <span style="color: #1565c0; line-height: 1.6;">
        â€¢ <code>rocprof --stats ./your_program;</code> - åŸºç¡€æ€§èƒ½åˆ†æ<br/>
        â€¢ <code>rocprof --sys-trace --hip-trace ./your_program;</code> - è¿½è¸ª HIP API å’Œ HSA è¿è¡Œæ—¶<br/>
        â€¢ <code>rocprof-plot -i results.csv -o report.html</code> - ç”Ÿæˆ HTML æŠ¥å‘Š<br/>
        â€¢ <code>rocprof --help</code> - æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹
      </span>
    </div>
  </div>
</div>
<div style="background: #fff3e0; border: 1px solid #ff9800; border-radius: 8px; padding: 16px; margin: 16px 0;">
  <div style="display: flex; align-items: start;">
    <span style="font-size: 20px; margin-right: 10px;">ğŸ¯</span>
    <div>
      <strong style="color: #ef6c00;">ä¼˜åŒ–å»ºè®®</strong><br>
      <span style="color: #ef6c00; line-height: 1.6;">
        â€¢ Occupancy > 70%ï¼šé€šå¸¸è¶³å¤Ÿï¼Œä¸å¿…è¿½æ±‚ 100%<br/>
        â€¢ å‡å°‘å¯„å­˜å™¨ä½¿ç”¨ï¼šç®€åŒ–è®¡ç®—ã€å¤ç”¨å˜é‡<br/>
        â€¢ å‡å° LDS ä½¿ç”¨ï¼šåˆç†è®¾è®¡ workgroup å¤§å°<br/>
        â€¢ ä½¿ç”¨ <code>--maxrregcount=N</code> ç¼–è¯‘é€‰é¡¹é™åˆ¶å¯„å­˜å™¨æ•°
      </span>
    </div>
  </div>
</div>

---

### 2.3.2 æ˜¾å­˜ (VRAM) vs å†…å­˜ (DRAM)ï¼šæ•°æ®æ¬è¿çš„"è¿‡è·¯è´¹"

GPU çš„å¦ä¸€ä¸ªç“¶é¢ˆæ˜¯**å†…å­˜å¸¦å®½**ã€‚

#### ğŸ’¾ HBM å¸¦å®½ä¸ºä»€ä¹ˆè¿™ä¹ˆè´µï¼Ÿ

| å†…å­˜ç±»å‹ | å¸¦å®½ | å»¶è¿Ÿ | å®¹é‡ | ä»·æ ¼ |
|:---|:---|:---|:---|:---|
| **DDR4 (CPU)** | ~25 GB/s | ~80 ns | 128 GB | ä¾¿å®œ |
| **GDDR6 (æ¸¸æˆæ˜¾å¡)** | ~500 GB/s | ~100 ns | 24 GB | ä¸­ç­‰ |
| **HBM3 (MI300X)** | **5.3 TB/s** | ~120 ns | 192 GB | éå¸¸è´µ |

#### ğŸš€ MI300X çš„ 5.3 TB/s æ˜¯ä»€ä¹ˆæ¦‚å¿µï¼Ÿ

- ç›¸å½“äºæ¯ç§’æ¬è¿ **1000 éƒ¨é«˜æ¸…ç”µå½±**
- æˆ–è€…æ¯ç§’æ¬è¿ **æ•´ä¸ªç»´åŸºç™¾ç§‘ï¼ˆè‹±æ–‡ç‰ˆï¼‰100 æ¬¡**

#### ğŸ”¬ ä¸ºä»€ä¹ˆ HBM è¿™ä¹ˆå¿«ï¼Ÿ

| æŠ€æœ¯ | è¯´æ˜ |
|:---|:---|
| **å †å å°è£…** | å†…å­˜èŠ¯ç‰‡ç›´æ¥å †åœ¨ GPU æ—è¾¹ |
| **è¶…å®½æ€»çº¿** | 4096 bitï¼ˆå¯¹æ¯” DDR4 çš„ 64 bitï¼‰ |
| **çŸ­è·ç¦»** | ä¿¡å·ä¼ è¾“è·¯å¾„åªæœ‰å‡ æ¯«ç±³ |

#### âš¡ PCIe Gen5 çš„ç“¶é¢ˆ

GPU å’Œ CPU ä¹‹é—´çš„æ•°æ®æ¬è¿é€šè¿‡ **PCIe** æ€»çº¿ï¼Œå®ƒçš„é€Ÿåº¦è¿œæ…¢äº HBMï¼š

| PCIe ç‰ˆæœ¬ | å¸¦å®½ (x16) | å•å‘æ¬è¿ 32 GB æ—¶é—´ |
|:---|:---|:---|
| **PCIe 4.0** | 32 GB/s | 1 ç§’ |
| **PCIe 5.0** | 64 GB/s | 0.5 ç§’ |

**å®é™…å½±å“**ï¼š

```python
# åœ¨ CPU å‡†å¤‡æ•°æ®
x = torch.randn(1024, 1024, 1024)  # 4 GB

# æ¬åˆ° GPU
x = x.cuda()  # PCIe 5.0 éœ€è¦ 0.06 ç§’

# åœ¨ GPU è®¡ç®—
y = x @ x.T  # GPU åªéœ€è¦å‡ æ¯«ç§’ï¼
```

> ğŸ¯ **å®æˆ˜æµ‹è¯•**ï¼šè®©æˆ‘ä»¬å®é™…æµ‹è¯•ä¸€ä¸‹ PCIe å’Œ GPU æ˜¾å­˜çš„å¸¦å®½å·®å¼‚ï¼

åˆ›å»ºæ–‡ä»¶ `bench_bandwidth.py`ï¼š

```python
# file: bench_bandwidth_final.py
import torch
import time
import numpy as np

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)

print("=== å†…å­˜å¸¦å®½æµ‹è¯• (Final) ===")

if not torch.cuda.is_available():
    print("é”™è¯¯: æœªæ£€æµ‹åˆ° GPU è®¾å¤‡ï¼")
    exit(1)

device_id = 0
device = torch.device(f"cuda:{device_id}")
props = torch.cuda.get_device_properties(device_id)

print(f"GPU è®¾å¤‡: {torch.cuda.get_device_name(device_id)}")
print(f"æ¶æ„: {props.name}")
print(f"æ€»æ˜¾å­˜: {props.total_memory / 1024**3:.2f} GB\n")

# ==========================================
# æµ‹è¯• 1: PCIe / æ€»çº¿å¸¦å®½ (CPU â†’ GPU)
# ==========================================
print("=== æµ‹è¯• 1: PCIe å¸¦å®½ (CPU â†’ GPU) ===")

# ä½¿ç”¨ pinned memory (é”é¡µå†…å­˜) ä»¥è·å¾—æœ€ä½³ä¼ è¾“æ€§èƒ½
# è¿™æ¨¡æ‹Ÿäº†é«˜æ€§èƒ½åº”ç”¨åœºæ™¯ä¸‹çš„çœŸå®ä¼ è¾“é€Ÿåº¦
sizes_mb = [1, 10, 100, 500, 1024, 2048]

for size_mb in sizes_mb:
    size_bytes = size_mb * 1024 * 1024
    data_cpu = torch.randn(size_bytes // 4, dtype=torch.float32).pin_memory()

    # é¢„çƒ­
    data_cpu.to(device, non_blocking=True)
    torch.cuda.synchronize()

    # æ­£å¼æµ‹è¯•
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)

    start_event.record()
    data_gpu = data_cpu.to(device, non_blocking=True)
    end_event.record()
    
    torch.cuda.synchronize()
    elapsed_ms = start_event.elapsed_time(end_event)

    bandwidth_gbs = (size_mb / 1024) / (elapsed_ms / 1000)
    print(f"æ•°æ®å¤§å°: {size_mb:4d} MB | æ—¶é—´: {elapsed_ms:8.2f} ms | å¸¦å®½: {bandwidth_gbs:6.2f} GB/s")

# ==========================================
# æµ‹è¯• 2: GPU æ˜¾å­˜å¸¦å®½ (GPU å†…éƒ¨)
# ==========================================
print("\n=== æµ‹è¯• 2: GPU æ˜¾å­˜å¸¦å®½ (GPU å†…éƒ¨) ===")

# é€‰å–è¾ƒå¤§çš„æ•°æ®å—ä»¥æµ‹è¯•æŒç»­å¸¦å®½
# å¦‚æœæ˜¾å­˜ä¸å¤Ÿå¤§ï¼Œè‡ªåŠ¨é™åˆ¶æœ€å¤§æµ‹è¯•è§„æ¨¡
test_sizes_mb = [100, 500, 1024]
max_safe_mb = int((props.total_memory / 1024**3) * 1024 * 0.4)
test_sizes_mb = [s for s in test_sizes_mb if s <= max_safe_mb]
if not test_sizes_mb: test_sizes_mb = [max_safe_mb]

for size_mb in test_sizes_mb:
    size_elements = (size_mb * 1024 * 1024) // 4
    src = torch.randn(size_elements, device=device, dtype=torch.float32)
    dst = torch.empty_like(src)
    
    # é¢„çƒ­
    dst.copy_(src)
    torch.cuda.synchronize()

    iterations = 50
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)

    start_event.record()
    for _ in range(iterations):
        dst.copy_(src)
    end_event.record()
    
    torch.cuda.synchronize()
    total_time_ms = start_event.elapsed_time(end_event)
    avg_time_ms = total_time_ms / iterations

    # å¸¦å®½ = (è¯» + å†™) / æ—¶é—´
    total_data_gb = (size_mb * 2) / 1024
    bandwidth_gbs = total_data_gb / (avg_time_ms / 1000)
    
    print(f"æ•°æ®å¤§å°: {size_mb:4d} MB | æ—¶é—´: {avg_time_ms:8.2f} ms | å¸¦å®½: {bandwidth_gbs:6.2f} GB/s")

# ==========================================
# æµ‹è¯• 3: è®¡ç®— vs æ¬è¿ (ä¼ è¾“ç“¶é¢ˆéªŒè¯)
# ==========================================
print("\n=== æµ‹è¯• 3: è®¡ç®—æ—¶é—´ vs æ•°æ®æ¬è¿æ—¶é—´ ===")

# å®šä¹‰ä¸€ä¸ªè¶³å¤Ÿå¤§çš„æ•°æ®é‡ (256MB)
data_size_mb = 256
N = 1024 * 1024 * (data_size_mb // 4)  # 256MB float32
data_cpu = torch.randn(N, dtype=torch.float32).pin_memory()

# 3.1 æµ‹è¯•æ•°æ®æ¬è¿ (PCIe/Bus)
start_event = torch.cuda.Event(enable_timing=True)
end_event = torch.cuda.Event(enable_timing=True)

# é¢„çƒ­
data_gpu = data_cpu.to(device, non_blocking=True)
torch.cuda.synchronize()

start_event.record()
data_gpu = data_cpu.to(device, non_blocking=True)
end_event.record()
torch.cuda.synchronize()
transfer_time_ms = start_event.elapsed_time(end_event)

# 3.2 æµ‹è¯•ç®€å•è®¡ç®— (ReLU - å†…å­˜å¯†é›†å‹)
# è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„ Memory Bound æ“ä½œï¼Œèƒ½å¾ˆå¥½åœ°å¯¹æ¯”ä¼ è¾“å’Œè®¡ç®—
# é¢„çƒ­
torch.relu(data_gpu)
torch.cuda.synchronize()

start_event.record()
# æ¨¡æ‹Ÿä¸€æ¬¡ç®€å•è®¡ç®—
result = torch.relu(data_gpu)
end_event.record()
torch.cuda.synchronize()
compute_time_ms = start_event.elapsed_time(end_event)

print(f"æ•°æ®æ¬è¿ (PCIe): {transfer_time_ms:.2f} ms")
print(f"ç®€å•è®¡ç®— (GPU):  {compute_time_ms:.2f} ms (ReLU)")

ratio = transfer_time_ms / compute_time_ms
print(f"\nâš ï¸ æ•°æ®æ¬è¿æ¯”è®¡ç®—æ…¢ {ratio:.1f} å€ï¼")
print("   (è¿™è¯æ˜äº†ï¼šé¢‘ç¹æ¬è¿æ•°æ®ä¼šæˆä¸ºç³»ç»Ÿç“¶é¢ˆ)")

# ==========================================
# ç†è®ºå¸¦å®½å¯¹æ¯”
# ==========================================
print("\n=== ç†è®ºå¸¦å®½å¯¹æ¯” ===")
print(f"PCIe 4.0 x16 ç†è®ºå¸¦å®½: 32 GB/s")
print(f"PCIe 5.0 x16 ç†è®ºå¸¦å®½: 64 GB/s")
print(f"LPDDR5X ç†è®ºå¸¦å®½:    ~270 GB/s (Radeon 8060S)")

print(f"\nå®é™… PCIe å¸¦å®½ä¼šå—å¤šä¸ªå› ç´ å½±å“:")
print("  - CPU å’Œ GPU çš„è´Ÿè½½æƒ…å†µ")
print("  - ç³»ç»Ÿå†…å­˜å¸¦å®½")
print("  - PCIe é“¾è·¯è´¨é‡å’Œåè®®å¼€é”€")

# æ¸…ç†å†…å­˜
del data_cpu, data_gpu, src, dst, result
torch.cuda.empty_cache()

```

**è¿è¡Œæµ‹è¯•**ï¼š

```bash
python bench_bandwidth.py
```

**è¾“å‡º**ï¼ˆåœ¨ Radeon 8060S ä¸Šï¼‰ï¼š

```
=== å†…å­˜å¸¦å®½æµ‹è¯• ===
GPU è®¾å¤‡: AMD Radeon 8060S
æ¶æ„: AMD Radeon 8060S
æ€»æ˜¾å­˜: 62.47 GB

=== æµ‹è¯• 1: PCIe å¸¦å®½ (CPU â†’ GPU) ===
æ•°æ®å¤§å°:    1 MB | æ—¶é—´:     0.29 ms | å¸¦å®½:   3.34 GB/s
æ•°æ®å¤§å°:   10 MB | æ—¶é—´:     0.29 ms | å¸¦å®½:  34.23 GB/s
æ•°æ®å¤§å°:  100 MB | æ—¶é—´:     1.22 ms | å¸¦å®½:  80.34 GB/s
æ•°æ®å¤§å°:  500 MB | æ—¶é—´:     6.11 ms | å¸¦å®½:  79.98 GB/s
æ•°æ®å¤§å°: 1024 MB | æ—¶é—´:    12.17 ms | å¸¦å®½:  82.19 GB/s
æ•°æ®å¤§å°: 2048 MB | æ—¶é—´:    24.79 ms | å¸¦å®½:  80.68 GB/s

=== æµ‹è¯• 2: GPU æ˜¾å­˜å¸¦å®½ (GPU å†…éƒ¨) ===
æ•°æ®å¤§å°:  100 MB | æ—¶é—´:     0.97 ms | å¸¦å®½: 201.87 GB/s
æ•°æ®å¤§å°:  500 MB | æ—¶é—´:     4.93 ms | å¸¦å®½: 197.89 GB/s
æ•°æ®å¤§å°: 1024 MB | æ—¶é—´:    10.19 ms | å¸¦å®½: 196.21 GB/s

=== æµ‹è¯• 3: è®¡ç®—æ—¶é—´ vs æ•°æ®æ¬è¿æ—¶é—´ ===
æ•°æ®æ¬è¿ (PCIe): 3.21 ms
ç®€å•è®¡ç®— (GPU):  2.43 ms (ReLU)

âš ï¸ æ•°æ®æ¬è¿æ¯”è®¡ç®—æ…¢ 1.3 å€ï¼
   (è¿™è¯æ˜äº†ï¼šé¢‘ç¹æ¬è¿æ•°æ®ä¼šæˆä¸ºç³»ç»Ÿç“¶é¢ˆ)

=== ç†è®ºå¸¦å®½å¯¹æ¯” ===
PCIe 4.0 x16 ç†è®ºå¸¦å®½: 32 GB/s
PCIe 5.0 x16 ç†è®ºå¸¦å®½: 64 GB/s
LPDDR5X ç†è®ºå¸¦å®½:    ~270 GB/s (Radeon 8060S)

å®é™… PCIe å¸¦å®½ä¼šå—å¤šä¸ªå› ç´ å½±å“:
  - CPU å’Œ GPU çš„è´Ÿè½½æƒ…å†µ
  - ç³»ç»Ÿå†…å­˜å¸¦å®½
  - PCIe é“¾è·¯è´¨é‡å’Œåè®®å¼€é”€
```

<div style="background: #ffebee; border: 1px solid #f44336; border-radius: 8px; padding: 16px; margin: 16px 0;">
  <div style="display: flex; align-items: start;">
    <span style="font-size: 20px; margin-right: 10px;">âš ï¸</span>
    <div>
      <strong style="color: #c62828;">å…³é”®å‘ç° (Strix Halo ç‰¹æœ‰)</strong><br>
      <span style="color: #c62828; line-height: 1.6;">
        ä»æµ‹è¯•ç»“æœå¯ä»¥çœ‹åˆ°ï¼š<br/>
        â€¢ <strong>Host-to-Device å¸¦å®½æƒŠäºº: ~80 GB/s</strong>ï¼ˆè¿œè¶… PCIe 4.0/5.0ï¼Œå¾—ç›Šäº APU ç»Ÿä¸€å†…å­˜æ¶æ„ï¼‰<br/>
        â€¢ <strong>GPU æ˜¾å­˜å¸¦å®½: ~200 GB/s</strong>ï¼ˆç¬¦åˆ LPDDR5X å››é€šé“ç†è®ºå€¼ï¼‰<br/>
        â€¢ <strong>æ•°æ®æ¬è¿ä¾ç„¶æ˜¯ç“¶é¢ˆ:</strong> å°½ç®¡æ‹¥æœ‰æé«˜çš„äº’è”å¸¦å®½ï¼Œæ•°æ®ä¼ è¾“è€—æ—¶ä»æ¯” GPU å†…éƒ¨è®¿å­˜/è®¡ç®—æ…¢ <strong>30% ä»¥ä¸Š</strong>ã€‚<br/>
        å› æ­¤ï¼Œå³ä½¿åœ¨é«˜æ€§èƒ½ APU ä¸Šï¼Œå‡å°‘ CPU-GPU æ•°æ®ä¼ è¾“ä¾ç„¶æ˜¯ä¼˜åŒ–çš„é»„é‡‘æ³•åˆ™ã€‚
      </span>
    </div>
  </div>
</div>

**è§£å†³æ–¹æ¡ˆ**ï¼š

| æ–¹æ¡ˆ | è¯´æ˜ |
|:---|:---|
| **æ•°æ®é¢„å–** | æå‰æŠŠæ•°æ®æ¬åˆ° GPU |
| **æµæ°´çº¿é‡å ** | è®¡ç®—å’Œæ•°æ®æ¬è¿åŒæ—¶è¿›è¡Œ |
| **æ¨¡å‹å¹¶è¡Œ** | æŠŠæ¨¡å‹åˆ†å¸ƒåˆ°å¤šä¸ª GPUï¼Œå‡å°‘è·¨å¡é€šä¿¡ |

#### ğŸ” æ•°æ®å±€éƒ¨æ€§ä¼˜åŒ–ï¼šå¦‚ä½•å‡å°‘ GPU é¥¿è‚šå­

GPU è®¡ç®—å¿«ï¼Œä½†éœ€è¦æŒç»­çš„æ•°æ®æµã€‚å¦‚æœæ•°æ®ä¾›åº”ä¸ä¸Šï¼ŒGPU å°±ä¼š"é¥¿è‚šå­"ã€‚

**é¥¥é¥¿ç¤ºä¾‹**ï¼š

```cpp
__global__ void starving(float* data) {
    int i = threadIdx.x;
    // æ¯æ¬¡è®¿é—®éƒ½å»æ˜¾å­˜æ‹¿ â†’ é«˜å»¶è¿Ÿ
    for (int j = 0; j < 100; j++) {
        data[i] += data[i + j * 1024];
    }
}
```

**ä¼˜åŒ–ï¼šä½¿ç”¨å…±äº«å†…å­˜ç¼“å­˜**

å½“ä½ éœ€è¦é‡å¤è®¿é—®åŒä¸€å—æ•°æ®æ—¶ï¼ŒLDS æ˜¯ç¥å™¨ã€‚

```cpp
__global__ void optimized(float* data) {
    __shared__ float s_data[256];  // ç‰‡ä¸Šå…±äº«å†…å­˜
    int i = threadIdx.x;

    // åªè®¿é—®ä¸€æ¬¡æ˜¾å­˜ï¼Œæ”¾åˆ°å…±äº«å†…å­˜
    s_data[i] = data[i];
    __syncthreads();  // ç­‰æ‰€æœ‰çº¿ç¨‹éƒ½åŠ è½½å®Œ

    // åç»­éƒ½ä»å…±äº«å†…å­˜è¯»ï¼ˆå¿«10å€ï¼‰
    for (int j = 0; j < 100; j++) {
        s_data[i] += s_data[(i + j) % 256];
    }

    data[i] = s_data[i];  // æœ€åå†™å›
}
```

<div style="background: #fffde7; border: 1px solid #fbc02d; border-radius: 8px; padding: 16px; margin: 16px 0;">
  <div style="display: flex; align-items: start;">
    <span style="font-size: 20px; margin-right: 10px;">âš¡</span>
    <div>
      <strong style="color: #f57f17;">è®¿é—®é€Ÿåº¦å¯¹æ¯”</strong><br>
      <span style="color: #e65100; line-height: 1.6;">
        â€¢ <strong>æ˜¾å­˜ (HBM)</strong>ï¼š100-200 å‘¨æœŸ <span style="color: #999; font-size: 0.9em;">(æ…¢)</span><br/>
        â€¢ <strong>å…±äº«å†…å­˜ (LDS)</strong>ï¼š10-20 å‘¨æœŸ <span style="color: #999; font-size: 0.9em;">(å¿« 10 å€)</span><br/>
        â€¢ <strong>å¯„å­˜å™¨ (VGPR)</strong>ï¼š1 å‘¨æœŸ <span style="color: #999; font-size: 0.9em;">(æå¿«)</span>
      </span>
    </div>
  </div>
</div>

**æ€§èƒ½æå‡**ï¼šé€šå¸¸èƒ½å¿« 2-10 å€ã€‚

---

## ğŸ“š æœ¬ç« å°ç»“

é€šè¿‡æœ¬ç« ï¼Œä½ å­¦ä¹ äº†ï¼š

| ä¸»é¢˜ | æ ¸å¿ƒè¦ç‚¹ |
|:---|:---|
| **è½¯ä»¶æ ˆ** | PyTorch ä»£ç å¦‚ä½•ç»è¿‡ HIP â†’ HSA â†’ Driver â†’ GPU æ‰§è¡Œ |
| **æ€ç»´è½¬å˜** | ä» CPU çš„"ä½å»¶è¿Ÿ"åˆ° GPU çš„"é«˜åå"ï¼ŒSIMT æ¨¡å‹çš„å·¥ä½œåŸç† |
| **ç¡¬ä»¶æ¶æ„** | AMD GPU çš„ CUã€LDSã€HBMï¼Œä»¥åŠå†…å­˜å¸¦å®½çš„é‡è¦æ€§ |

**ä¸‹ä¸€æ­¥**ï¼šåœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä¼˜åŒ– GPU ç®—å­ï¼Œå……åˆ†åˆ©ç”¨è¿™äº›ç¡¬ä»¶ç‰¹æ€§ã€‚

---

## ğŸ¯ å®æˆ˜ç»ƒä¹ 

é€šè¿‡æœ¬ç« å­¦ä¹ ï¼Œä½ å·²ç»è·å¾—äº†å¤šä¸ªå®Œæ•´çš„æµ‹è¯•ç¨‹åºã€‚ç°åœ¨è¯·åŠ¨æ‰‹å®è·µï¼š

### ğŸ“ å¿…åšç»ƒä¹ 

1. **è¿½è¸ªä¾èµ–**
   ```bash
   # ç”¨ ldd æŸ¥çœ‹ä½ ç¯å¢ƒä¸­çš„ PyTorch
   TORCH_LIB=$(python -c "import torch; print(torch.__file__)" | sed 's/__init__.py/lib/')
   ldd $TORCH_LIB/libtorch_python.so | grep -E "amd|hip|hsa"
   ```

2. **æ‰‹å†™ HIP**
   - è¿è¡Œ `hello_rocm.cpp`ï¼ŒéªŒè¯ GPU èƒ½æ­£å¸¸å·¥ä½œ
   - å°è¯•ä¿®æ”¹æ ¸å‡½æ•°å®ç°å‘é‡ä¹˜æ³•è€ŒéåŠ æ³•

3. **æ€§èƒ½å¯¹æ¯”æµ‹è¯•**
   ```bash
   # åœ¨ä½ çš„ Radeon 8060S ä¸Šè¿è¡Œ ResNet-50 æ€§èƒ½æµ‹è¯•
   python bench_resnet.py
   
   # æŸ¥çœ‹åŠ é€Ÿæ¯”ï¼ŒéªŒè¯ GPU vs CPU çš„æ€§èƒ½å·®å¼‚
   ```

4. **åˆ†æ”¯å‘æ•£æµ‹è¯•**
   ```bash
   # ç¼–è¯‘å¹¶è¿è¡Œåˆ†æ”¯å‘æ•£æ€§èƒ½æµ‹è¯•
   hipcc bench_divergence.cpp -o bench_divergence -O3
   ./bench_divergence
   
   # è§‚å¯Ÿåˆ†æ”¯å‘æ•£å¯¼è‡´çš„æ€§èƒ½ä¸‹é™
   ```

5. **å†…å­˜å¸¦å®½æµ‹è¯•**
   ```bash
   # æµ‹è¯• PCIe å’Œ GPU æ˜¾å­˜å¸¦å®½
   python bench_bandwidth.py
   
   # ç†è§£æ•°æ®æ¬è¿å¯¹æ€§èƒ½çš„å½±å“
   ```

### ğŸš€ è¿›é˜¶ç»ƒä¹ 

6. **Occupancy åˆ†æ**
   ```bash
   # ç¼–è¯‘æµ‹è¯•ç¨‹åº
   hipcc test_occupancy.cpp -o test_occupancy -O3
   
   # è¿è¡Œåˆ†æï¼ˆä¼šç”Ÿæˆ results.csvï¼‰
   rocprof --stats ./test_occupancy
   
   #æŸ¥çœ‹ç”Ÿæˆçš„ CSV æ–‡ä»¶
   cat results.csv
   ```

7. **ä¼˜åŒ–æŒ‘æˆ˜**
   - å°è¯•ä¼˜åŒ– `bench_divergence.cpp` ä¸­çš„ä»£ç ï¼Œæ¶ˆé™¤åˆ†æ”¯å‘æ•£
   - å°è¯•ä¼˜åŒ– `test_occupancy.cpp` ä¸­çš„å¯„å­˜å™¨ä½¿ç”¨ï¼Œæé«˜ Occupancy
   - å¯¹æ¯”ä¼˜åŒ–å‰åçš„æ€§èƒ½å·®å¼‚

### ğŸ“Š å®éªŒæŠ¥å‘Šæ¨¡æ¿

å®Œæˆç»ƒä¹ åï¼Œå¡«å†™ä»¥ä¸‹æŠ¥å‘Šï¼š

| æµ‹è¯•é¡¹ | é¢„æœŸç»“æœ | å®é™…ç»“æœ | å·®å¼‚åˆ†æ |
|:---|:---|:---|:---|
| **GPU ä¿¡æ¯** | gfx1151 | `____` | |
| **ResNet-50 åŠ é€Ÿæ¯”** | >50x | `____`x | |
| **åˆ†æ”¯å‘æ•£å½±å“** | ~2x | `____`x | |
| **PCIe å¸¦å®½** | ~7 GB/s | `____` GB/s | |
| **GPU æ˜¾å­˜å¸¦å®½** | >500 GB/s | `____` GB/s | |
| **Occupancy** | >70% | `____`% | |

---

## ğŸ“– å‚è€ƒèµ„æº

| èµ„æº | é“¾æ¥ |
|:---|:---|
| **ROCm Documentation** | [https://rocm.docs.amd.com/](https://rocm.docs.amd.com/) |
| **AMD GPU Architecture** | [https://www.amd.com/en/products/graphics/data-center-gpus](https://www.amd.com/en/products/graphics/data-center-gpus) |
| **HIP Programming Guide** | [https://rocm.docs.amd.com/projects/HIP/en/latest/](https://rocm.docs.amd.com/projects/HIP/en/latest/) |
| **LLVM AMDGPU Backend** | [https://llvm.org/docs/AMDGPUUsage.html](https://llvm.org/docs/AMDGPUUsage.html) |
